[
  {
    "objectID": "posts.html",
    "href": "posts.html",
    "title": "My Blog",
    "section": "",
    "text": "Order By\n       Default\n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n         \n          Title\n        \n         \n          Author\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n\nDate\n\n\nTitle\n\n\nAuthor\n\n\n\n\n\n\nOct 31, 2024\n\n\nRunning Jupyter Notebooks on Amarel’s GPU Nodes\n\n\nKai Tan\n\n\n\n\nMay 1, 2024\n\n\nAccelerating Your Python Code: Tips for Faster Performance\n\n\nKai Tan\n\n\n\n\nMay 1, 2024\n\n\nLearning PyTorch: Neural Networks\n\n\nKai Tan\n\n\n\n\nApr 22, 2024\n\n\nIntro to PyTorch DataLoader\n\n\nKai Tan\n\n\n\n\nApr 21, 2024\n\n\nPyTorch: Logistic Regression\n\n\nKai Tan\n\n\n\n\nApr 20, 2024\n\n\nLearning PyTorch: Simple Linear Regression\n\n\nKai Tan\n\n\n\n\nApr 1, 2024\n\n\nUsing SLURM Clusters for Python Jobs\n\n\nKai Tan\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/Pytorch_dataloader.html",
    "href": "posts/Pytorch_dataloader.html",
    "title": "Intro to PyTorch DataLoader",
    "section": "",
    "text": "In PyTorch, the DataLoader is a powerful tool that lets you iterate over datasets efficiently. It handles batching, shuffling, and even multi-process data loading — all crucial for model training."
  },
  {
    "objectID": "posts/Pytorch_dataloader.html#what-is-a-dataloader",
    "href": "posts/Pytorch_dataloader.html#what-is-a-dataloader",
    "title": "Intro to PyTorch DataLoader",
    "section": "",
    "text": "In PyTorch, the DataLoader is a powerful tool that lets you iterate over datasets efficiently. It handles batching, shuffling, and even multi-process data loading — all crucial for model training."
  },
  {
    "objectID": "posts/Pytorch_dataloader.html#step-1-create-a-custom-dataset",
    "href": "posts/Pytorch_dataloader.html#step-1-create-a-custom-dataset",
    "title": "Intro to PyTorch DataLoader",
    "section": "Step 1: Create a Custom Dataset",
    "text": "Step 1: Create a Custom Dataset\nStart by subclassing torch.utils.data.Dataset. Here’s an example where we simulate binary labels.\n\nimport torch\nfrom torch.utils.data import Dataset\n\nclass MyDataset(Dataset):\n    def __init__(self):\n        self.data = torch.arange(100)\n        self.labels = self.data % 2  # binary labels\n\n    def __len__(self):\n        return len(self.data)\n\n    def __getitem__(self, idx):\n        return self.data[idx], self.labels[idx]"
  },
  {
    "objectID": "posts/Pytorch_dataloader.html#step-2-use-dataloader-to-load-in-batches",
    "href": "posts/Pytorch_dataloader.html#step-2-use-dataloader-to-load-in-batches",
    "title": "Intro to PyTorch DataLoader",
    "section": "Step 2: Use DataLoader to Load in Batches",
    "text": "Step 2: Use DataLoader to Load in Batches\n\nfrom torch.utils.data import DataLoader\n\ndataset = MyDataset()\ndataloader = DataLoader(dataset, batch_size=10, shuffle=True)\n\nYou can now easily loop through the dataset in batches:\n\nfor batch_idx, (x, y) in enumerate(dataloader):\n    if batch_idx &lt; 2: \n        print(f\"Batch {batch_idx}\")\n        print(\"x:\", x)\n        print(\"y:\", y)\n        print(\"---\")\n\nBatch 0\nx: tensor([58, 82, 22, 98, 81, 68, 62, 24, 69, 20])\ny: tensor([0, 0, 0, 0, 1, 0, 0, 0, 1, 0])\n---\nBatch 1\nx: tensor([77, 32, 52, 97, 41, 93, 27, 65, 33, 39])\ny: tensor([1, 0, 0, 1, 1, 1, 1, 1, 1, 1])\n---"
  },
  {
    "objectID": "posts/Pytorch_dataloader.html#step-3-use-a-real-world-dataset-california-housing",
    "href": "posts/Pytorch_dataloader.html#step-3-use-a-real-world-dataset-california-housing",
    "title": "Intro to PyTorch DataLoader",
    "section": "Step 3: Use a Real-World Dataset (California Housing)",
    "text": "Step 3: Use a Real-World Dataset (California Housing)\nYou can also use built-in datasets from torchvision.\n\nfrom sklearn.datasets import fetch_california_housing\nfrom sklearn.preprocessing import StandardScaler\nfrom torch.utils.data import TensorDataset\n\n# Load data\ncalifornia = fetch_california_housing()\nX = california.data[:1000]\ny = california.target[:1000]\n\n# Convert to tensors\nX_tensor = torch.tensor(X, dtype=torch.float32)\ny_tensor = torch.tensor(y, dtype=torch.float32).view(-1, 1)\n\n# Wrap in a dataset\nreal_dataset = TensorDataset(X_tensor, y_tensor)\nreal_loader = DataLoader(real_dataset, batch_size=32, shuffle=True)\n\nPreview the first batch:\n\nfeatures, targets = next(iter(real_loader))\nprint(features.shape)  # [32, 8]\nprint(targets.shape)   # [32, 1]\n\ntorch.Size([32, 8])\ntorch.Size([32, 1])"
  },
  {
    "objectID": "posts/Pytorch_dataloader.html#step-4-visualization",
    "href": "posts/Pytorch_dataloader.html#step-4-visualization",
    "title": "Intro to PyTorch DataLoader",
    "section": "Step 4: Visualization",
    "text": "Step 4: Visualization\n\nimport matplotlib.pyplot as plt\n\nfeature_index = 0  # e.g., Median Income\nplt.hist(features[:, feature_index].numpy(), bins=20, edgecolor='k')\nplt.title(\"Distribution of Feature: Median Income\")\nplt.xlabel(\"Value\")\nplt.ylabel(\"Frequency\")\nplt.grid(True)\nplt.show()"
  },
  {
    "objectID": "posts/Pytorch_dataloader.html#common-dataloader-parameters",
    "href": "posts/Pytorch_dataloader.html#common-dataloader-parameters",
    "title": "Intro to PyTorch DataLoader",
    "section": "Common DataLoader Parameters",
    "text": "Common DataLoader Parameters\n\n\n\nParameter\nMeaning\n\n\n\n\nbatch_size\nNumber of samples per batch\n\n\nshuffle\nWhether to shuffle data at every epoch\n\n\nnum_workers\nNumber of subprocesses to use for data loading\n\n\ndrop_last\nDrop last batch if it’s smaller than batch_size\n\n\npin_memory\nSpeed up data transfer to GPU if using CUDA"
  },
  {
    "objectID": "posts/Pytorch_dataloader.html#reproducibility-tip",
    "href": "posts/Pytorch_dataloader.html#reproducibility-tip",
    "title": "Intro to PyTorch DataLoader",
    "section": "Reproducibility Tip",
    "text": "Reproducibility Tip\nTo make shuffling deterministic:\ng = torch.Generator()\ng.manual_seed(42)\n\ndataloader = DataLoader(dataset, batch_size=10, shuffle=True, generator=g)"
  },
  {
    "objectID": "posts/Pytorch_dataloader.html#summary",
    "href": "posts/Pytorch_dataloader.html#summary",
    "title": "Intro to PyTorch DataLoader",
    "section": "Summary",
    "text": "Summary\nPyTorch’s DataLoader is a flexible and powerful abstraction for handling dataset loading. Whether you’re using built-in datasets or your own, DataLoader gives you:\n\nMini-batch loading\nShuffling for randomness\nParallelism with num_workers"
  },
  {
    "objectID": "posts/PyTroch_Simple_Linear_Regression.html",
    "href": "posts/PyTroch_Simple_Linear_Regression.html",
    "title": "Learning PyTorch: Simple Linear Regression",
    "section": "",
    "text": "In this post, I will demonstrate how to implement a simple linear regression model using PyTorch."
  },
  {
    "objectID": "posts/PyTroch_Simple_Linear_Regression.html#solve-linear-regression-by-gradient-descent-in-numpy",
    "href": "posts/PyTroch_Simple_Linear_Regression.html#solve-linear-regression-by-gradient-descent-in-numpy",
    "title": "Learning PyTorch: Simple Linear Regression",
    "section": "Solve Linear Regression by Gradient Descent in NumPy",
    "text": "Solve Linear Regression by Gradient Descent in NumPy\n\nStep 1: Given weight \\(w\\) and bias \\(b\\), compute prediction (Forward pass): \\[\\hat{y}_{i} = b + w x_{i}\\]\nStep 2: compute the loss: \\[\nL(w,b)\n= \\frac1n \\sum_{i=1}^n (y_i - \\hat y_i)^2\n= \\frac1n \\sum_{i=1}^n (y_i - b - w * x_i)^2\n\\]\nStep 3: compute the gradients (backward pass): \\[\n\\frac{\\partial L}{\\partial w}\n= -\\frac{2}{n} \\sum_{i=1}^n (y_i - b - w x_i) x_i\n= -\\frac{2}{n} \\sum_{i=1}^n (y_i - \\hat y_i) x_i\n\\] \\[\n\\frac{\\partial L}{\\partial b}\n= -\\frac{2}{n} \\sum_{i=1}^n (y_i - b - w x_i)\n= -\\frac{2}{n} \\sum_{i=1}^n (y_i - \\hat y_i)\n\\]\nStep 4: update the parameters: \\[\nw \\leftarrow w - \\eta \\frac{\\partial L}{\\partial w}\\] \\[\nb \\leftarrow b - \\eta \\frac{\\partial L}{\\partial b}\n\\]\n\n\n# initialization of weight and bias\nw = rng.standard_normal(1)\nb = rng.standard_normal(1)\nprint('Initial bias:', b)\nprint('Initial weight:', w)\n\nlr = 0.01 # learning rate\nn_epochs = 1000 # number of iterations\nfor t in range(n_epochs):\n    # step 1: prediction\n    y_pred = b + w * x\n    # step 2: compute loss\n    loss = np.mean((y - y_pred) ** 2)\n    # step 3: compute gradients\n    dw = -2 * np.mean(x * (y - y_pred))\n    db = -2 * np.mean(y - y_pred)\n    # step4: update weights and bias\n    w -= lr * dw\n    b -= lr * db\nprint('Final bias:', b)\nprint('Final weight:', w)\n\n# plot the results\nplt.plot(x, y, 'b.')\nplt.plot(x, b + x * w, 'r-')\nplt.xlabel('x')\nplt.ylabel('y')\nplt.title('Linear Regression using Gradient Descent')\nplt.show()\n\nInitial bias: [-0.14451867]\nInitial weight: [-0.67517827]\nFinal bias: [1.00716318]\nFinal weight: [1.99588476]"
  },
  {
    "objectID": "posts/PyTroch_Simple_Linear_Regression.html#solve-linear-regression-with-pytorch-autograd",
    "href": "posts/PyTroch_Simple_Linear_Regression.html#solve-linear-regression-with-pytorch-autograd",
    "title": "Learning PyTorch: Simple Linear Regression",
    "section": "Solve Linear Regression with PyTorch (autograd)",
    "text": "Solve Linear Regression with PyTorch (autograd)\n\n# using pytorch\nimport torch\nimport torch.optim as optim\nimport torch.nn as nn\n\ntorch.manual_seed(42)  # for reproducibility\n\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n# convert data to torch tensors\nx_tensor = torch.from_numpy(x).float().to(device)\ny_tensor = torch.from_numpy(y).float().to(device)\n\nlr = 0.1\nn_epochs = 1000\n# initial weights and bias\nb = torch.randn(1, requires_grad=True, device=device)\nw = torch.randn(1, requires_grad=True, device=device)\n\nfor t in range(n_epochs):\n    # step 1: prediction\n    y_hat = b + w * x_tensor\n    # step 2: compute loss\n    error = y_tensor - y_hat\n    loss = torch.mean(error ** 2)\n    # step 3: compute gradients (via autograd)\n    loss.backward()\n    # step 4: update weights and bias\n    with torch.no_grad():\n        b -= lr * b.grad\n        w -= lr * w.grad\n    b.grad.zero_()\n    w.grad.zero_()\nprint('Final bias (PyTorch):', b)\nprint('Final weight (PyTorch):', w)\n\nFinal bias (PyTorch): tensor([1.0072], requires_grad=True)\nFinal weight (PyTorch): tensor([1.9959], requires_grad=True)"
  },
  {
    "objectID": "posts/PyTroch_Simple_Linear_Regression.html#solve-linear-regression-with-pytorch-autograd-loss-optimizer",
    "href": "posts/PyTroch_Simple_Linear_Regression.html#solve-linear-regression-with-pytorch-autograd-loss-optimizer",
    "title": "Learning PyTorch: Simple Linear Regression",
    "section": "Solve Linear Regression with PyTorch (autograd + loss + optimizer)",
    "text": "Solve Linear Regression with PyTorch (autograd + loss + optimizer)\n\nb = torch.randn(1, requires_grad=True, dtype=torch.float, device=device)\nw = torch.randn(1, requires_grad=True, dtype=torch.float, device=device)\n\nlr = 0.01\nn_epochs = 1000\nloss_fn = nn.MSELoss(reduction='mean')\noptimizer = optim.SGD([b, w], lr=lr)\nfor t in range(n_epochs):\n    # step 1: prediction\n    y_hat = b + w * x_tensor\n    # step 2: compute loss\n    loss = loss_fn(y_tensor, y_hat)    \n    # step 3: clear old and compute current gradients\n    optimizer.zero_grad() # clear old gradients\n    loss.backward()       # compute current gradients\n    # step 4: update weights and bias using the specified optimizer (e.g. SGD)\n    optimizer.step()\n    \nprint('Final bias (PyTorch with optim):', b)\nprint('Final weight (PyTorch with optim):', w)\n\nFinal bias (PyTorch with optim): tensor([1.0072], requires_grad=True)\nFinal weight (PyTorch with optim): tensor([1.9959], requires_grad=True)"
  },
  {
    "objectID": "posts/PyTroch_Simple_Linear_Regression.html#object-oriented-programming",
    "href": "posts/PyTroch_Simple_Linear_Regression.html#object-oriented-programming",
    "title": "Learning PyTorch: Simple Linear Regression",
    "section": "Object-oriented programming",
    "text": "Object-oriented programming\n\nimport torch\nimport torch.nn as nn\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\nclass LinearRegression(nn.Module):\n    def __init__(self):\n        super().__init__()\n        # Initialize parameters\n        self.b = nn.Parameter(torch.randn(1, device=device)) \n        self.w = nn.Parameter(torch.randn(1, device=device)) \n\n    def forward(self, x):\n        # Forward pass: compute predicted y\n        return self.b + self.w * x \n\n# Create model\nmodel = LinearRegression().to(device)\n# define the loss function and optimizer\nlr = 0.01\nn_epochs = 1000\nloss_fn = nn.MSELoss(reduction='mean')\noptimizer = optim.SGD(model.parameters(), lr=lr)\n\nfor t in range(n_epochs):\n    model.train()\n    # step 1: makd prediction (forward pass)\n    y_hat = model(x_tensor)\n    # step 2: compute loss\n    loss = loss_fn(y_tensor, y_hat)\n    # step 3: clear old and compute current gradients\n    optimizer.zero_grad() # clear old gradients\n    loss.backward()       # compute current gradients\n    # step 4: update parameters via an optimizer (e.g. SGD)\n    optimizer.step() \n\nprint(model.state_dict())\nprint('Final bias (class + PyTorch):', model.b.item())\nprint('Final weight (class + PyTorch):', model.w.item())\n\nOrderedDict({'b': tensor([1.0072]), 'w': tensor([1.9959])})\nFinal bias (class + PyTorch): 1.0071604251861572\nFinal weight (class + PyTorch): 1.9958817958831787"
  },
  {
    "objectID": "posts/PyTroch_Simple_Linear_Regression.html#create-linear-model-via-nn.sequential",
    "href": "posts/PyTroch_Simple_Linear_Regression.html#create-linear-model-via-nn.sequential",
    "title": "Learning PyTorch: Simple Linear Regression",
    "section": "Create Linear Model via nn.Sequential",
    "text": "Create Linear Model via nn.Sequential\n\nlr = 0.01\nn_epochs = 1000\nmodel = nn.Sequential(nn.Linear(1, 1)).to(device)\nloss_fn = nn.MSELoss(reduction='mean')\noptimizer = optim.SGD(model.parameters(), lr=lr)\n\nlosses = []\nfor t in range(n_epochs):\n    model.train()\n    # step 1: make prediction (forward pass)\n    y_hat = model(x_tensor)\n    # step 2: compute loss\n    loss = loss_fn(y_tensor, y_hat)\n    losses.append(loss.item())\n    # step 3: clear old and compute current gradients\n    optimizer.zero_grad() # clear old gradients\n    loss.backward()       # compute current gradients\n    # step 4: update parameters via an optimizer (e.g. SGD)\n    optimizer.step() \n\nprint(model.state_dict())\nplt.plot(losses)\nplt.xlabel('Epochs')\nplt.ylabel('Loss')\nplt.title('Training Loss over Epochs')\nplt.show()\n\nOrderedDict({'0.weight': tensor([[1.9959]]), '0.bias': tensor([1.0072])})"
  },
  {
    "objectID": "posts/PyTroch_Simple_Linear_Regression.html#dataset-and-data-loader",
    "href": "posts/PyTroch_Simple_Linear_Regression.html#dataset-and-data-loader",
    "title": "Learning PyTorch: Simple Linear Regression",
    "section": "Dataset and Data Loader",
    "text": "Dataset and Data Loader\n\nfrom torch.utils.data import Dataset, TensorDataset\nclass myDataset(Dataset):\n    def __init__(self, x, y):\n        self.x = x_tensor\n        self.y = y_tensor\n    def __getitem__(self, index):\n        return self.x[index], self.y[index]\n    \n    def __len__(self):\n        return len(self.x)\n\ntrain_data = myDataset(x_tensor, y_tensor)\nprint('Length of train_data:', len(train_data))\nprint('First item in train_data:', train_data[0])\n\ntrain_data1 = TensorDataset(x_tensor, y_tensor)\nprint('Length of train_data1:', len(train_data1))\nprint('First item in train_data1:', train_data1[0])\n\nLength of train_data: 1000\nFirst item in train_data: (tensor([0.4967]), tensor([2.1334]))\nLength of train_data1: 1000\nFirst item in train_data1: (tensor([0.4967]), tensor([2.1334]))"
  },
  {
    "objectID": "posts/PyTroch_Simple_Linear_Regression.html#solve-linear-regression-with-dataset-and-data-loader",
    "href": "posts/PyTroch_Simple_Linear_Regression.html#solve-linear-regression-with-dataset-and-data-loader",
    "title": "Learning PyTorch: Simple Linear Regression",
    "section": "Solve Linear Regression with Dataset and Data Loader",
    "text": "Solve Linear Regression with Dataset and Data Loader\n\nfrom torch.utils.data import DataLoader\ntrain_loader = DataLoader(train_data, batch_size=50, shuffle=True)\nlr = 0.01\nn_epochs = 1000\nmodel = nn.Sequential(nn.Linear(1, 1)).to(device)\nloss_fn = nn.MSELoss(reduction='mean')\noptimizer = optim.SGD(model.parameters(), lr=lr)\nfor t in range(n_epochs):\n    for x_batch, y_batch in train_loader:\n        x_batch = x_batch.to(device)\n        y_batch = y_batch.to(device)\n        model.train()\n        # step 1: make prediction (forward pass)\n        y_hat = model(x_batch)\n        # step 2: compute loss\n        loss = loss_fn(y_batch, y_hat)\n        # step 3: clear old and compute current gradients\n        optimizer.zero_grad() # clear old gradients\n        loss.backward()       # compute current gradients\n        # step 4: update parameters via an optimizer (e.g. SGD)\n        optimizer.step()       \nprint(model.state_dict())\n\nOrderedDict({'0.weight': tensor([[1.9958]]), '0.bias': tensor([1.0073])})"
  },
  {
    "objectID": "posts/PyTroch_Simple_Linear_Regression.html#evaluation-in-validation-set",
    "href": "posts/PyTroch_Simple_Linear_Regression.html#evaluation-in-validation-set",
    "title": "Learning PyTorch: Simple Linear Regression",
    "section": "Evaluation in validation set",
    "text": "Evaluation in validation set\n\nfrom torch.utils.data import random_split\ndataset = TensorDataset(x_tensor, y_tensor)\ntrain_dataset, val_dataset = random_split(dataset, [800, 200])\ntrain_loader = DataLoader(train_dataset, batch_size=40)\nval_loader = DataLoader(val_dataset, batch_size=20)\nlosses = []\nval_losses = []\n\nfor t in range(n_epochs):\n    for x_batch, y_batch in train_loader:\n        x_batch = x_batch.to(device)\n        y_batch = y_batch.to(device)\n\n        model.train()\n        # step 1: make prediction (forward pass)\n        y_hat = model(x_batch)\n        # step 2: compute loss\n        loss = loss_fn(y_batch, y_hat)\n        # step 3: clear old and compute current gradients\n        optimizer.zero_grad() # clear old gradients\n        loss.backward()       # compute current gradients\n        # step 4: update parameters via an optimizer (e.g. SGD)\n        optimizer.step() \n        \n        # store the loss for plotting later\n        losses.append(loss)\n    \n    # validation step (no need to track gradients)\n    with torch.no_grad():\n        for x_val, y_val in val_loader:\n            model.eval()\n            # step 1: prediction\n            y_hat = model(x_val.to(device))\n            # step 2: compute loss\n            val_loss = loss_fn(y_val.to(device), y_hat)\n            val_losses.append(val_loss.item())\nprint(model.state_dict())\n\nOrderedDict({'0.weight': tensor([[1.9960]]), '0.bias': tensor([1.0072])})"
  },
  {
    "objectID": "posts/PyTroch_Simple_Linear_Regression.html#summary",
    "href": "posts/PyTroch_Simple_Linear_Regression.html#summary",
    "title": "Learning PyTorch: Simple Linear Regression",
    "section": "Summary",
    "text": "Summary\n\nNo need to manually compute gradients; PyTorch’s autograd does it for you.\nUse torch.nn for loss functions and optimizers.\nUse nn.Sequential for building models in a modular way.\nUse torch.utils.data.Dataset and DataLoader for data handling.\nUse validation sets to evaluate model performance.\nReference"
  },
  {
    "objectID": "posts/Pytorch_Logistic_Regression.html",
    "href": "posts/Pytorch_Logistic_Regression.html",
    "title": "PyTorch: Logistic Regression",
    "section": "",
    "text": "The logistic regression model estimates the probability of the label \\(y\\) given the input \\(x\\) by:\n\\[\n\\mathbb{P}(y = 1) = \\sigma(w^\\top x + b)\n\\]\nHere:\n\n\\(x \\in \\mathbb{R}^d\\) is the input vector, \\(y \\in \\{0, 1\\}\\) is the binary label\n\n\\(w \\in \\mathbb{R}^d\\) is the weight vector, \\(b \\in \\mathbb{R}\\) is the bias (intercept)\n\n\\(\\sigma(z) = \\dfrac{1}{1 + e^{-z}} = \\dfrac{e^z}{1 + e^z}\\) is the sigmoid function"
  },
  {
    "objectID": "posts/Pytorch_Logistic_Regression.html#logistic-regression-model",
    "href": "posts/Pytorch_Logistic_Regression.html#logistic-regression-model",
    "title": "PyTorch: Logistic Regression",
    "section": "",
    "text": "The logistic regression model estimates the probability of the label \\(y\\) given the input \\(x\\) by:\n\\[\n\\mathbb{P}(y = 1) = \\sigma(w^\\top x + b)\n\\]\nHere:\n\n\\(x \\in \\mathbb{R}^d\\) is the input vector, \\(y \\in \\{0, 1\\}\\) is the binary label\n\n\\(w \\in \\mathbb{R}^d\\) is the weight vector, \\(b \\in \\mathbb{R}\\) is the bias (intercept)\n\n\\(\\sigma(z) = \\dfrac{1}{1 + e^{-z}} = \\dfrac{e^z}{1 + e^z}\\) is the sigmoid function"
  },
  {
    "objectID": "posts/Pytorch_Logistic_Regression.html#binary-cross-entropy-loss",
    "href": "posts/Pytorch_Logistic_Regression.html#binary-cross-entropy-loss",
    "title": "PyTorch: Logistic Regression",
    "section": "Binary Cross-Entropy Loss",
    "text": "Binary Cross-Entropy Loss\nThe loss function we minimize is the binary cross-entropy:\n\\[\nL(w, b) = -\\frac{1}{n} \\sum_{i=1}^n \\left[ y_i \\log(\\hat{y}_i) + (1 - y_i) \\log(1 - \\hat{y}_i) \\right]\n\\]\nHere:\n\n\\(n\\) is the number of training examples\n\\(y_i\\) is the true label for the \\(i\\)-th sample\n\\(\\hat{y}_i = \\sigma(w^\\top x_i + b)\\) is the predicted probability\n\n\nIntuition\n\nIf \\(y_i = \\hat{y}_i = 1\\) for all \\(i\\), the loss is 0 — perfect prediction.\nIf \\(y_i = \\hat{y}_i = 0\\) for all \\(i\\), the loss is also 0.\nIf \\(y_i = 1\\), \\(\\hat{y}_i = 0\\), the loss becomes infinite — the worst-case prediction."
  },
  {
    "objectID": "posts/Pytorch_Logistic_Regression.html#deriving-the-gradient",
    "href": "posts/Pytorch_Logistic_Regression.html#deriving-the-gradient",
    "title": "PyTorch: Logistic Regression",
    "section": "Deriving the Gradient",
    "text": "Deriving the Gradient\nWe start with the individual loss for each sample:\n\\[\nL_i = -\\left[ y_i \\log(\\sigma(z_i)) + (1 - y_i) \\log(1 - \\sigma(z_i))\n\\right],\n\\]\nwhere \\(z_i = w^\\top x_i + b\\). Using the identity \\(\\frac{d\\sigma(z)}{dz} = \\sigma(z)(1 - \\sigma(z))\\), we obtain the gradient:\n\\[\\begin{align*}\n\\frac{\\partial L_i}{\\partial z_i}\n&= -\\left[ y_i \\frac{1}{\\sigma(z_i)} \\frac{\\partial \\sigma(z_i)}{\\partial z_i} + (1 - y_i) \\frac{-1}{1 - \\sigma(z_i)} \\frac{\\partial \\sigma(z_i)}{\\partial z_i} \\right]\\\\\n&= -\\left[ y_i \\bigl(1 - \\sigma(z_i)\\bigr) - (1 - y_i) \\sigma(z_i) \\right]\\\\\n&= \\sigma(z_i) - y_i\\\\\n&= \\hat{y}_i - y_i.\n\\end{align*}\\]\nBy the chain rule, we can compute the gradient of the loss w.r.t. the weights \\(w\\) and bias \\(b\\): \\[\\begin{align*}\n\\frac{\\partial L_i}{\\partial w} &= \\frac{\\partial L_i}{\\partial z_i} \\cdot \\frac{\\partial z_i}{\\partial w} = (\\hat y_i - y_i) x_i,\\\\\n\\frac{\\partial L_i}{\\partial b} &= \\frac{\\partial L_i}{\\partial z_i} \\cdot \\frac{\\partial z_i}{\\partial b} = (\\hat y_i - y_i).\n\\end{align*}\\]\nAveraging over all \\(n\\) samples,the gradients become:\n\\[\\begin{equation}\n\\frac{\\partial L}{\\partial w} = \\frac{1}{n} \\sum_{i=1}^n (\\hat{y}_i - y_i) x_i\n\\quad \\text{ and } \\quad\n\\frac{\\partial L}{\\partial b} = \\frac{1}{n} \\sum_{i=1}^n (\\hat{y}_i - y_i).\n\\end{equation}\\]"
  },
  {
    "objectID": "posts/Pytorch_Logistic_Regression.html#step-1-data-generation",
    "href": "posts/Pytorch_Logistic_Regression.html#step-1-data-generation",
    "title": "PyTorch: Logistic Regression",
    "section": "Step 1: Data Generation",
    "text": "Step 1: Data Generation\n\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nnp.random.seed(0)\nn, d = 1000, 2\nX = np.random.randn(n, d)\ntrue_w = np.array([2.0, -3.0])\nbias = 0.5\n\nlogits = X @ true_w + bias\nprobs = 1 / (1 + np.exp(-logits))\ny = (probs &gt; 0.5).astype(np.float32)\n\nplt.scatter(X[:, 0], X[:, 1], c=y, cmap='bwr', edgecolor='k')\nplt.title(\"Synthetic Binary Classification Data\")\nplt.xlabel(\"x1\")\nplt.ylabel(\"x2\")\nplt.grid(True)\nplt.show()"
  },
  {
    "objectID": "posts/Pytorch_Logistic_Regression.html#step-2-logistic-regression-with-numpy",
    "href": "posts/Pytorch_Logistic_Regression.html#step-2-logistic-regression-with-numpy",
    "title": "PyTorch: Logistic Regression",
    "section": "Step 2: Logistic Regression with NumPy",
    "text": "Step 2: Logistic Regression with NumPy\n\ndef sigmoid(z):\n    return 1 / (1 + np.exp(-z))\n\ndef binary_cross_entropy(y_true, y_pred):\n    eps = 1e-7\n    return -np.mean(y_true * np.log(y_pred + eps) + (1 - y_true) * np.log(1 - y_pred + eps))\n\nw = np.zeros(2)\nb = 0.0\nlr = 0.1\n\nfor epoch in range(100):\n    z = X @ w + b\n    y_pred = sigmoid(z)\n    loss = binary_cross_entropy(y, y_pred)\n\n    dz = y_pred - y\n    dw = X.T @ dz / n\n    db = np.mean(dz)\n\n    w -= lr * dw\n    b -= lr * db\n\n    if epoch % 10 == 0:\n        print(f\"Epoch {epoch}, Loss: {loss:.4f}\")\n\n# Plotting the decision boundary\nx_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1\nx_vals = np.linspace(x_min, x_max, 100)\n# Solve for x2 on the decision boundary line: w1*x1 + w2*x2 + b = 0\n# =&gt; x2 = -(w1*x1 + b)/w2\ny_vals = -(w[0] * x_vals + b) / w[1]\n\n# Plot\nplt.figure(figsize=(6, 4))\nplt.scatter(X[:, 0], X[:, 1], c=y, cmap='bwr', edgecolor='k')\nplt.plot(x_vals, y_vals, 'k--', label='Decision Boundary')\nplt.title(\"Logistic Regression Decision Boundary\")\nplt.xlabel(\"x1\")\nplt.ylabel(\"x2\")\nplt.legend()\nplt.grid(True)\nplt.show()\n\nEpoch 0, Loss: 0.6931\nEpoch 10, Loss: 0.5710\nEpoch 20, Loss: 0.4932\nEpoch 30, Loss: 0.4403\nEpoch 40, Loss: 0.4019\nEpoch 50, Loss: 0.3726\nEpoch 60, Loss: 0.3495\nEpoch 70, Loss: 0.3305\nEpoch 80, Loss: 0.3147\nEpoch 90, Loss: 0.3013"
  },
  {
    "objectID": "posts/Pytorch_Logistic_Regression.html#step-3-logistic-regression-with-pytorch",
    "href": "posts/Pytorch_Logistic_Regression.html#step-3-logistic-regression-with-pytorch",
    "title": "PyTorch: Logistic Regression",
    "section": "Step 3: Logistic Regression with PyTorch",
    "text": "Step 3: Logistic Regression with PyTorch\n\nimport torch\nfrom torch import nn\n\nX_tensor = torch.tensor(X, dtype=torch.float32)\ny_tensor = torch.tensor(y, dtype=torch.float32).view(-1, 1)\n\nmodel = nn.Sequential(\n    nn.Linear(d, 1),\n    nn.Sigmoid()\n)\n\nloss_fn = nn.BCELoss()\noptimizer = torch.optim.SGD(model.parameters(), lr=0.1)\n\nfor epoch in range(100):\n    model.train()  # set model to training mode\n    # step 1: make prediction (forward pass)\n    y_pred = model(X_tensor)\n    # step 2: compute loss\n    loss = loss_fn(y_pred, y_tensor)\n    # step 3: clear old and compute current gradients\n    optimizer.zero_grad() # clear old gradients\n    loss.backward()       # compute current gradients\n    # step 4: update parameters\n    optimizer.step()\n\n    if epoch % 10 == 0:\n        print(f\"Epoch {epoch}, Loss: {loss.item():.4f}\")\n\nEpoch 0, Loss: 0.6921\nEpoch 10, Loss: 0.5684\nEpoch 20, Loss: 0.4893\nEpoch 30, Loss: 0.4356\nEpoch 40, Loss: 0.3970\nEpoch 50, Loss: 0.3678\nEpoch 60, Loss: 0.3449\nEpoch 70, Loss: 0.3263\nEpoch 80, Loss: 0.3108\nEpoch 90, Loss: 0.2977"
  },
  {
    "objectID": "posts/blog_efficient_compute.html",
    "href": "posts/blog_efficient_compute.html",
    "title": "Accelerating Your Python Code: Tips for Faster Performance",
    "section": "",
    "text": "In this blog post, I will explore several techniques to accelerate your Python code and enhance its performance."
  },
  {
    "objectID": "posts/blog_efficient_compute.html#use-broadcasting-to-avoid-creating-a-diagonal-matrix",
    "href": "posts/blog_efficient_compute.html#use-broadcasting-to-avoid-creating-a-diagonal-matrix",
    "title": "Accelerating Your Python Code: Tips for Faster Performance",
    "section": "Use broadcasting to avoid creating a diagonal matrix",
    "text": "Use broadcasting to avoid creating a diagonal matrix\n\nUse A * v instead of A @ np.diag(v)\nUse v[:, np.newaxis] * A instead of np.diag(v) @ A\n\n\nimport numpy as np\nimport timeit\nA = np.random.randn(1000, 1000)\nv = np.random.randn(1000)\nassert np.all( A @ np.diag(v) == A * v )\nassert np.all( np.diag(v) @ A == v[:, np.newaxis] * A )\nprint('time for method 1:', timeit.timeit(lambda: A @ np.diag(v), number=10))\nprint('time for method 2:', timeit.timeit(lambda: A * v, number=10))\n\ntime for method 1: 0.18757016599999998\ntime for method 2: 0.005711124999999928"
  },
  {
    "objectID": "posts/blog_efficient_compute.html#avoid-large-matrix-multiplication",
    "href": "posts/blog_efficient_compute.html#avoid-large-matrix-multiplication",
    "title": "Accelerating Your Python Code: Tips for Faster Performance",
    "section": "Avoid large matrix multiplication",
    "text": "Avoid large matrix multiplication\n\nUse np.einsum('ij,ji-&gt;', A, B) instead of np.trace(A @ B)\n\n\nimport numpy as np\nimport timeit\nA = np.random.randn(1000, 1000)\nB = np.random.randn(1000, 1000)\nassert np.isclose(np.trace(A @ B), np.einsum('ij,ji-&gt;', A, B))\nprint('time for method 1:', timeit.timeit(lambda: np.trace(A @ B), number=10))\nprint('time for method 2:', timeit.timeit(lambda: np.einsum('ij,ji-&gt;', A, B), number=10))\n\ntime for method 1: 0.1677121669999999\ntime for method 2: 0.010563332999999897"
  },
  {
    "objectID": "posts/blog_efficient_compute.html#prioritize-the-order-of-matrix-multiplication",
    "href": "posts/blog_efficient_compute.html#prioritize-the-order-of-matrix-multiplication",
    "title": "Accelerating Your Python Code: Tips for Faster Performance",
    "section": "Prioritize the order of matrix multiplication",
    "text": "Prioritize the order of matrix multiplication\n\nUse A @ (B @ v) instead of A @ B @ v when v is a vector or an array of smaller size.\n\n\nimport numpy as np\nimport timeit\nA = np.random.randn(1000, 1000)\nB = np.random.randn(1000, 1000)\nv = np.random.randn(1000)\nassert np.allclose(A @ B @ v, A @ (B @ v))\nprint('time for method 1:', timeit.timeit(lambda: A @ B @ v, number=10))\nprint('time for method 2:', timeit.timeit(lambda: A @ (B @ v), number=10))\n\ntime for method 1: 0.15922845799999985\ntime for method 2: 0.0014511249999999976"
  },
  {
    "objectID": "teaching.html",
    "href": "teaching.html",
    "title": "Teaching",
    "section": "",
    "text": "Term\nCourse Title\nCourse Level\n\n\n\n\nSummer 2022\nProbability and Stochastic Processes\nFirst-year PhD\n\n\nSpring 2020\nStatistical Methods and Motivations\nUndergraduate\n\n\nFall 2019\nStatistical Methods and Motivations\nUndergraduate\n\n\n\n\n\n\n\n\n\n\n\n\n\nTerm\nCourse Title\nCourse Level\n\n\n\n\nFall 2024\nSTAT 534 Stat Learning for Data Science\nMaster\n\n\nSpring 2024\nSTAT 597 Data Wrangling\nMaster\n\n\nFall 2022\nSTAT 592 Theory of Probability\nPhD\n\n\nSpring 2021\nSTAT 401 Basic Statistics For Research\nUndergraduate\n\n\nSpring 2021\nSTAT 211 Statistics I\nUndergraduate\n\n\nFall 2020\nSTAT 401 Basic Statistics For Research\nUndergraduate\n\n\nFall 2020\nSTAT 211 Statistics I\nUndergraduate\n\n\nSpring 2019\nSTA 296 Statistical Methods and Motivations\nUndergraduate\n\n\nFall 2018\nSTA 296 Statistical Methods and Motivations\nUndergraduate"
  },
  {
    "objectID": "teaching.html#independent-instructor",
    "href": "teaching.html#independent-instructor",
    "title": "Teaching",
    "section": "",
    "text": "Term\nCourse Title\nCourse Level\n\n\n\n\nSummer 2022\nProbability and Stochastic Processes\nFirst-year PhD\n\n\nSpring 2020\nStatistical Methods and Motivations\nUndergraduate\n\n\nFall 2019\nStatistical Methods and Motivations\nUndergraduate\n\n\n\n\n\n\n\n\n\n\n\n\n\nTerm\nCourse Title\nCourse Level\n\n\n\n\nFall 2024\nSTAT 534 Stat Learning for Data Science\nMaster\n\n\nSpring 2024\nSTAT 597 Data Wrangling\nMaster\n\n\nFall 2022\nSTAT 592 Theory of Probability\nPhD\n\n\nSpring 2021\nSTAT 401 Basic Statistics For Research\nUndergraduate\n\n\nSpring 2021\nSTAT 211 Statistics I\nUndergraduate\n\n\nFall 2020\nSTAT 401 Basic Statistics For Research\nUndergraduate\n\n\nFall 2020\nSTAT 211 Statistics I\nUndergraduate\n\n\nSpring 2019\nSTA 296 Statistical Methods and Motivations\nUndergraduate\n\n\nFall 2018\nSTA 296 Statistical Methods and Motivations\nUndergraduate"
  },
  {
    "objectID": "research.html",
    "href": "research.html",
    "title": "Research",
    "section": "",
    "text": "Uncertainty quantification for iterative algorithms in linear models with application to early stopping\nPierre C. Bellec, Kai Tan (2024) (alphabetical order)\nMajor revision at the Annals of Statistics.\nPrecise Asymptotics of Bagging Regularized M-estimators\nTakuya Koriyama, Pratik Patil, Jin-Hong Du, Kai Tan, Pierre C. Bellec (2024)\nMajor revision at the Annals of Statistics."
  },
  {
    "objectID": "research.html#preprints",
    "href": "research.html#preprints",
    "title": "Research",
    "section": "",
    "text": "Uncertainty quantification for iterative algorithms in linear models with application to early stopping\nPierre C. Bellec, Kai Tan (2024) (alphabetical order)\nMajor revision at the Annals of Statistics.\nPrecise Asymptotics of Bagging Regularized M-estimators\nTakuya Koriyama, Pratik Patil, Jin-Hong Du, Kai Tan, Pierre C. Bellec (2024)\nMajor revision at the Annals of Statistics."
  },
  {
    "objectID": "research.html#publications",
    "href": "research.html#publications",
    "title": "Research",
    "section": "Publications",
    "text": "Publications\n\nEstimating Generalization Performance Along the Trajectory of Proximal SGD in Robust Regression\nKai Tan, Pierre C. Bellec (2024)\nAdvances in Neural Information Processing Systems (NeurIPS 2024). [arXiv][Code]\nCorrected generalized cross-validation for finite ensembles of penalized estimators\nPierre C. Bellec, Jin-Hong Du, Takuya Koriyama, Pratik Patil, Kai Tan (2023) (alphabetical order).\nJournal of the Royal Statistical Society Series B, to appear. [arXiv] [Code]\nMultinomial Logistic Regression: Asymptotic Normality on Null Covariates in High-Dimensions\nKai Tan, and Pierre C. Bellec (2023).\nAdvances in Neural Information Processing Systems 36 (2024). [arXiv] [Code]\nNoise covariance estimation in multi-task high-dimensional linear models\nKai Tan, Gabriel Romon, and Pierre C. Bellec (2023).\nBernoulli. 30.3 (2024): 1695-1722. [arXiv] [slides] [Code]\nFréchet sufficient variable selection with graphical structure among predictors.\nJiaying Weng \\(^*\\) , Kai Tan \\(^*\\) (\\(^*\\)=equal contribution), Cheng Wang and Zhou Yu (2025), Scandinavian Journal of Statistics.\nVariable dependent partial dimension reduction\nLu Li, Kai Tan, Xuerong Meggie Wen, and Zhou Yu (2023) (alphabetical order).\nTEST. 32, 521–541 (2023).\nSparse SIR: optimal rates and adaptive estimation\nKai Tan, Lei Shi, and Zhou Yu (2020).\nThe Annals of Statistics 48.1 (2020): 64-85.\nCOM-negative binomial distribution: modeling overdispersion and ultrahigh zero-inflated count data\nHuiming Zhang, Kai Tan and Bo Li (2018).\nFrontiers of Mathematics in China, 13(4), 967-998.\nSummaries of three keynote lectures at the SAE-2018\nKai Tan, and Lyu Ni (2018).\nStatistical Theory and Related Fields 2.2 (2018): 215-218.\nInterview with Professor Danny Pfeffermann\nLyu Ni, and Kai Tan (2018).\nStatistical Theory and Related Fields 2.2 (2018): 219-221."
  },
  {
    "objectID": "posts/running-jupyter-amarel.html",
    "href": "posts/running-jupyter-amarel.html",
    "title": "Running Jupyter Notebooks on Amarel’s GPU Nodes",
    "section": "",
    "text": "This tutorial provides a step-by-step guide on running a Jupyter Notebook on Amarel’s GPU nodes at Rutgers. Follow the instructions below to set up your Jupyter environment and access it from your local machine."
  },
  {
    "objectID": "posts/running-jupyter-amarel.html#overview",
    "href": "posts/running-jupyter-amarel.html#overview",
    "title": "Running Jupyter Notebooks on Amarel’s GPU Nodes",
    "section": "",
    "text": "This tutorial provides a step-by-step guide on running a Jupyter Notebook on Amarel’s GPU nodes at Rutgers. Follow the instructions below to set up your Jupyter environment and access it from your local machine."
  },
  {
    "objectID": "posts/running-jupyter-amarel.html#prerequisites",
    "href": "posts/running-jupyter-amarel.html#prerequisites",
    "title": "Running Jupyter Notebooks on Amarel’s GPU Nodes",
    "section": "Prerequisites",
    "text": "Prerequisites\n\nYou need access to the Rutgers Amarel cluster, you can request access at OARC website.\nYou need to use VPN to connect to the Rutgers network if you are off-campus."
  },
  {
    "objectID": "posts/running-jupyter-amarel.html#step-1-ssh-into-amarel",
    "href": "posts/running-jupyter-amarel.html#step-1-ssh-into-amarel",
    "title": "Running Jupyter Notebooks on Amarel’s GPU Nodes",
    "section": "Step 1: SSH into Amarel",
    "text": "Step 1: SSH into Amarel\nStart by opening your terminal and connecting to Amarel using SSH. Replace &lt;netid&gt; with your actual NetID:\nssh &lt;netid&gt;@amarel.rutgers.edu"
  },
  {
    "objectID": "posts/running-jupyter-amarel.html#step-2-start-an-interactive-session-on-a-gpu-node",
    "href": "posts/running-jupyter-amarel.html#step-2-start-an-interactive-session-on-a-gpu-node",
    "title": "Running Jupyter Notebooks on Amarel’s GPU Nodes",
    "section": "Step 2: Start an Interactive Session on a GPU Node",
    "text": "Step 2: Start an Interactive Session on a GPU Node\nOnce logged into Amarel, initiate an interactive session on a GPU node by running:\nsrun -N 1 --gres=gpu:1 --partition=gpu --mem=4G --time=02:00:00 --constraint=ampere --pty bash\n\nExplanation:\n\n-N 1: Requests 1 node.\n--gres=gpu:1: Allocates 1 GPU.\n--partition=gpu: Specifies the GPU partition.\n--mem=4G: Requests 4GB of memory.\n--time=02:00:00: Allocates 2 hours for the session.\n--constraint=ampere: Requests an Ampere GPU (e.g., NVIDIA A100).\n--pty bash: Opens a bash shell in the allocated session. This command will allocate the requested resources and start an interactive session."
  },
  {
    "objectID": "posts/running-jupyter-amarel.html#step-3-start-jupyter-notebook-server",
    "href": "posts/running-jupyter-amarel.html#step-3-start-jupyter-notebook-server",
    "title": "Running Jupyter Notebooks on Amarel’s GPU Nodes",
    "section": "Step 3: Start Jupyter Notebook Server",
    "text": "Step 3: Start Jupyter Notebook Server\nEnsure that Jupyter Notebook is installed in your Python environment on Amarel. You can use the community available Anaconda distribution by\nmodule use /projects/community/modulefiles\nmodule load anaconda/2024.06-ts840\nStart the Jupyter Notebook server with:\njupyter notebook --no-browser --ip=0.0.0.0 --port=8889\nIn the prompted message, note the node on which your job is running (e.g., gpu123)."
  },
  {
    "objectID": "posts/running-jupyter-amarel.html#step-4-set-up-ssh-port-forwarding",
    "href": "posts/running-jupyter-amarel.html#step-4-set-up-ssh-port-forwarding",
    "title": "Running Jupyter Notebooks on Amarel’s GPU Nodes",
    "section": "Step 4: Set Up SSH Port Forwarding",
    "text": "Step 4: Set Up SSH Port Forwarding\nTo connect your local machine to the Jupyter Notebook server running on Amarel, open a NEW terminal and set up an SSH tunnel:\nssh -L 9000:&lt;node_name&gt;:8889 &lt;netid&gt;@amarel.hpc.rutgers.edu\n\nReplace &lt;node_name&gt; with the node name from the interactive session (e.g., gpu123).\nReplace &lt;netid&gt; with your actual NetID.\n\n\nExplanation:\n\n-L 9000:&lt;node_name&gt;:8889: Forwards the port 8889 from Amarel’s compute node to localhost:9000 on your local machine."
  },
  {
    "objectID": "posts/running-jupyter-amarel.html#step-5-access-jupyter-notebook-from-your-browser",
    "href": "posts/running-jupyter-amarel.html#step-5-access-jupyter-notebook-from-your-browser",
    "title": "Running Jupyter Notebooks on Amarel’s GPU Nodes",
    "section": "Step 5: Access Jupyter Notebook from Your Browser",
    "text": "Step 5: Access Jupyter Notebook from Your Browser\nOpen your web browser and enter:\nlocalhost:9000\nThis will open the Jupyter Notebook interface that is running on Amarel’s compute node.\n\nIf you are prompted for a token, copy the token from the terminal where your Jupyter Notebook is running.\nYou can now create or upload notebooks and run them on the GPU resources provided."
  },
  {
    "objectID": "posts/running-jupyter-amarel.html#acknowledgments",
    "href": "posts/running-jupyter-amarel.html#acknowledgments",
    "title": "Running Jupyter Notebooks on Amarel’s GPU Nodes",
    "section": "Acknowledgments",
    "text": "Acknowledgments\nThis tutorial was adapted from the guidance provided by Robert Palmere, Senior Scientist at the Office of Advanced Research Computing (OARC) at Rutgers University. I greatly appreciate the help from Robert."
  },
  {
    "objectID": "posts/PyTorch_Neural_Networks.html",
    "href": "posts/PyTorch_Neural_Networks.html",
    "title": "Learning PyTorch: Neural Networks",
    "section": "",
    "text": "Introduction\nIn this blog post, we will explore how to build a neural network using PyTorch. PyTorch is a powerful and flexible deep learning framework that makes it easy to define, train, and evaluate neural networks. We will cover the following steps:\n\nSetting up the environment\nDefining the neural network\nPreparing the data\nTraining the network\nEvaluating the network\n\n\n\nSetting Up the Environment\nFirst, let’s ensure that we have PyTorch installed. You can install PyTorch using pip:\npip install torch torchvision\n\n\nDefining the Neural Network\nNext, we will define a simple neural network with five layers. Each hidden layer will use the ReLU activation function.\n\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\n\nclass NeuralNetwork(nn.Module):\n    def __init__(self, input_size):\n        super(NeuralNetwork, self).__init__()\n        self.fc1 = nn.Linear(input_size, 16)\n        self.fc2 = nn.Linear(16, 8)\n        self.fc3 = nn.Linear(8, 1)\n        \n    def forward(self, x):\n        x = torch.relu(self.fc1(x))\n        x = torch.relu(self.fc2(x))\n        x = self.fc3(x)\n        return x\n\n\n\nPreparing the Data\nFor this example, we will create a simple dataset. In a real-world scenario, you would use a dataset from a file or an online source.\n\nimport torch\nimport numpy as np\nfrom sklearn.datasets import fetch_california_housing\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import mean_squared_error, r2_score\nfrom sklearn.preprocessing import StandardScaler\n\n# Set random seed for reproducibility\nseed = 42\ntorch.manual_seed(seed)\nnp.random.seed(seed)\ntorch.cuda.manual_seed_all(seed)\ntorch.backends.cudnn.deterministic = True\ntorch.backends.cudnn.benchmark = False\n\n# Set device\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(f\"Using device: {device}\")\n\n# Load the California housing dataset (first 100 samples for simplicity)\ncalifornia = fetch_california_housing()\nX, y = california.data[:100], california.target[:100]\n\n# Split into train/test sets\nX_train, X_test, y_train, y_test = train_test_split(\n    X, y, test_size=0.2, random_state=seed\n)\n\n# Standardize features\nscaler = StandardScaler()\nX_train = scaler.fit_transform(X_train)\nX_test = scaler.transform(X_test)\n\n# Convert data to PyTorch tensors and move to device\nX_train_tensor = torch.from_numpy(X_train).float().to(device)\nX_test_tensor = torch.from_numpy(X_test).float().to(device)\ny_train_tensor = torch.from_numpy(y_train).float().unsqueeze(1).to(device)\ny_test_tensor = torch.from_numpy(y_test).float().unsqueeze(1).to(device)\n\n# Create a DataLoader for batching\ndataset = torch.utils.data.TensorDataset(X_train_tensor, y_train_tensor)\ndataloader = torch.utils.data.DataLoader(dataset, batch_size=20, shuffle=True)\n\nUsing device: cpu\n\n\n\n\nTraining the Network\nNow, let’s define the training function and train our neural network.\n\n# Train the neural network model\ninput_size = X_train.shape[1]\nmodel = NeuralNetwork(input_size)\nloss_fn = nn.MSELoss()\noptimizer = optim.Adam(model.parameters(), lr=0.01)\nn_epochs = 100\n\nfor epoch in range(n_epochs):\n    model.train()\n    for batch_X, batch_y in dataloader:\n        # step 1: make prediction (forward pass)\n        outputs = model(batch_X)\n        # step 2: compute loss\n        loss = loss_fn(outputs, batch_y)\n        # step 3: clear old and compute current gradients\n        optimizer.zero_grad() # clear old gradients\n        loss.backward()       # compute current gradients\n        # step 4: update parameters\n        optimizer.step()\n\n\n\nEvaluating the trained Network\nAfter training, we need to evaluate our network’s performance on the test set.\n\n# Evaluate the model\nmodel.eval()\nwith torch.no_grad():\n    y_pred_nn = model(X_test_tensor).numpy()\n\nmse_nn = mean_squared_error(y_test, y_pred_nn)\nr2_nn = r2_score(y_test, y_pred_nn)\n\nprint(f'MSE (Neural Network): {mse_nn}')\nprint(f'R2 (Neural Network): {r2_nn}')\n\nMSE (Neural Network): 0.143800164172336\nR2 (Neural Network): 0.8298946833143164\n\n\n\n\nConclusion\nIn this blog post, we covered the basics of building a neural network using PyTorch. We went through setting up the environment, defining the neural network, preparing the data, training the network, and evaluating its performance. PyTorch provides a flexible and intuitive framework for developing deep learning models, making it a popular choice for researchers and practitioners alike.\nFeel free to experiment with different architectures, hyperparameters, and datasets to further enhance your understanding and build more complex models.\n\n\nOnline Resources on PyTorch and Deep Learning\n\nPyTorch Documentation\nDeep Learning with PyTorch: A 60 Minute Blitz\nLearning PyTorch with Examples\nLearn PyTorch for Deep Learning: Zero to Mastery\nDeep Learning Book\nUnderstanding Deep Learning\nThe Little Book of Deep Learning\nFast.ai Course\nDive into Deep Learning\nStanford CS230: Deep Learning\nStanford CS231n: Convolutional Neural Networks for Visual Recognition\nMIT Deep Learning course\nUC Berkeley CS182\nExplain Backpropagation\nVisualizing machine learning\nPlayground for tensorflow"
  },
  {
    "objectID": "posts/blog_slurm.html",
    "href": "posts/blog_slurm.html",
    "title": "Using SLURM Clusters for Python Jobs",
    "section": "",
    "text": "High-performance computing (HPC) clusters are essential for handling large-scale computations in various scientific and engineering fields. SLURM (Simple Linux Utility for Resource Management) is a widely-used workload manager designed for high-performance computing clusters. In this blog post, I’ll guide you through the process of using SLURM to run Python jobs efficiently on an HPC cluster."
  },
  {
    "objectID": "posts/blog_slurm.html#step-1-load-required-modules",
    "href": "posts/blog_slurm.html#step-1-load-required-modules",
    "title": "Using SLURM Clusters for Python Jobs",
    "section": "Step 1: Load Required Modules",
    "text": "Step 1: Load Required Modules\nOn many HPC systems, you need to load specific modules before you can use certain software. For example, to load Python:\nmodule load python/3.9.6"
  },
  {
    "objectID": "posts/blog_slurm.html#step-2-create-a-virtual-environment",
    "href": "posts/blog_slurm.html#step-2-create-a-virtual-environment",
    "title": "Using SLURM Clusters for Python Jobs",
    "section": "Step 2: Create a Virtual Environment",
    "text": "Step 2: Create a Virtual Environment\nIt’s good practice to create a virtual environment for your project to manage dependencies.\npython -m venv myenv\nsource myenv/bin/activate"
  },
  {
    "objectID": "posts/blog_slurm.html#step-3-install-required-packages",
    "href": "posts/blog_slurm.html#step-3-install-required-packages",
    "title": "Using SLURM Clusters for Python Jobs",
    "section": "Step 3: Install Required Packages",
    "text": "Step 3: Install Required Packages\nInstall the necessary Python packages using pip.\npip install numpy pandas scikit-learn matplotlib"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Kai Tan",
    "section": "",
    "text": "Welcome to my website! I’m currently a Stein Fellow in Department of Statistics at Stanford University. Previous, I completed my Ph.D. in Statistics at Rutgers University, under the supervision of Prof. Pierre C. Bellec Pierre C. Bellec.\nMy research interests span a wide range of areas in Statistics and Machine Learning, including high dimensional statistical inference, dimension reduction, and causal inference. Specifically, I develop tools for uncertainty quantification of iterative algorithms (e.g. estimating prediction risks and constructing confidence intervals for parameters of interest), to improve the reliability of statistical machine learning methods."
  },
  {
    "objectID": "index.html#recent-news",
    "href": "index.html#recent-news",
    "title": "Kai Tan",
    "section": "Recent news",
    "text": "Recent news\n\n[Nov. 2024]: The paper Uncertainty quantification for iterative algorithms in linear models with application to early stopping has been invited for a major revision at the Annals of Statistics.\n[Sep. 2024]: The paper Estimating Generalization Performance Along the Trajectory of Proximal SGD in Robust Regression is accepted to NeurIPS 2024.\n[Aug. 2024]: The paper Corrected generalized cross-validation for finite ensembles of penalized estimators is accepted to the Journal of the Royal Statistical Society, Series B (Statistical Methodology).\n[Jun. 2024]: Presented a poster at DIMACS Workshop on Modeling Randomness in Neural Network Training.\n[Sep. 2023]: The paper Multinomial Logistic Regression: Asymptotic Normality on Null Covariates in High-Dimensions is accepted to NeurIPS 2023.\n[May 2023]: I am honored to receive the Gold Medal of student poster competition at the Conference on Recent Advances in Statistics and Data Science, with a Celebration of Professors Regina Liu and Cun-Hui Zhang’s Special Birthdays.\n[March 2023]: I am honored to receive the IMS Hannan Graduate Student Travel Award.\n[Jan. 2023]: I am honored to receive the Travel Award at the 2023 Statistics Annual Winter Workshop titled “Modern Computational Statistics” hosted by the statistics department at the University of Florida."
  }
]