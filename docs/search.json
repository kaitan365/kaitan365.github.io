[
  {
    "objectID": "posts.html",
    "href": "posts.html",
    "title": "My Blog",
    "section": "",
    "text": "Order By\n       Default\n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n         \n          Title\n        \n         \n          Author\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n\nDate\n\n\nTitle\n\n\nAuthor\n\n\n\n\n\n\nOct 31, 2024\n\n\nRunning Jupyter Notebooks on Amarel’s GPU Nodes\n\n\nKai Tan\n\n\n\n\nMay 1, 2024\n\n\nAccelerating Your Python Code: Tips for Faster Performance\n\n\nKai Tan\n\n\n\n\nMay 1, 2024\n\n\nLearning PyTorch: Neural Networks\n\n\nKai Tan\n\n\n\n\nApr 20, 2024\n\n\nLearning PyTorch: Simple Linear Regression\n\n\nKai Tan\n\n\n\n\nApr 1, 2024\n\n\nUsing SLURM Clusters for Python Jobs\n\n\nKai Tan\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/blog_slurm.html",
    "href": "posts/blog_slurm.html",
    "title": "Using SLURM Clusters for Python Jobs",
    "section": "",
    "text": "High-performance computing (HPC) clusters are essential for handling large-scale computations in various scientific and engineering fields. SLURM (Simple Linux Utility for Resource Management) is a widely-used workload manager designed for high-performance computing clusters. In this blog post, I’ll guide you through the process of using SLURM to run Python jobs efficiently on an HPC cluster."
  },
  {
    "objectID": "posts/blog_slurm.html#step-1-load-required-modules",
    "href": "posts/blog_slurm.html#step-1-load-required-modules",
    "title": "Using SLURM Clusters for Python Jobs",
    "section": "Step 1: Load Required Modules",
    "text": "Step 1: Load Required Modules\nOn many HPC systems, you need to load specific modules before you can use certain software. For example, to load Python:\nmodule load python/3.9.6"
  },
  {
    "objectID": "posts/blog_slurm.html#step-2-create-a-virtual-environment",
    "href": "posts/blog_slurm.html#step-2-create-a-virtual-environment",
    "title": "Using SLURM Clusters for Python Jobs",
    "section": "Step 2: Create a Virtual Environment",
    "text": "Step 2: Create a Virtual Environment\nIt’s good practice to create a virtual environment for your project to manage dependencies.\npython -m venv myenv\nsource myenv/bin/activate"
  },
  {
    "objectID": "posts/blog_slurm.html#step-3-install-required-packages",
    "href": "posts/blog_slurm.html#step-3-install-required-packages",
    "title": "Using SLURM Clusters for Python Jobs",
    "section": "Step 3: Install Required Packages",
    "text": "Step 3: Install Required Packages\nInstall the necessary Python packages using pip.\npip install numpy pandas scikit-learn matplotlib"
  },
  {
    "objectID": "posts/PyTorch_Neural_Networks.html",
    "href": "posts/PyTorch_Neural_Networks.html",
    "title": "Learning PyTorch: Neural Networks",
    "section": "",
    "text": "Introduction\nIn this blog post, we will explore how to build a neural network using PyTorch. PyTorch is a powerful and flexible deep learning framework that makes it easy to define, train, and evaluate neural networks. We will cover the following steps:\n\nSetting up the environment\nDefining the neural network\nPreparing the data\nTraining the network\nEvaluating the network\n\n\n\nSetting Up the Environment\nFirst, let’s ensure that we have PyTorch installed. You can install PyTorch using pip:\npip install torch torchvision\n\n\nDefining the Neural Network\nNext, we will define a simple neural network with five layers. Each hidden layer will use the ReLU activation function.\n\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\n\nclass NeuralNetwork(nn.Module):\n    def __init__(self, input_size):\n        super(NeuralNetwork, self).__init__()\n        self.fc1 = nn.Linear(input_size, 16)\n        self.fc2 = nn.Linear(16, 8)\n        self.fc3 = nn.Linear(8, 1)\n        \n    def forward(self, x):\n        x = torch.relu(self.fc1(x))\n        x = torch.relu(self.fc2(x))\n        x = self.fc3(x)\n        return x\n\n\n\nPreparing the Data\nFor this example, we will create a simple dataset. In a real-world scenario, you would use a dataset from a file or an online source.\n\nimport torch\nimport numpy as np\nfrom sklearn.datasets import fetch_california_housing\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import mean_squared_error, r2_score\nfrom sklearn.preprocessing import StandardScaler\n\n# Set random seed for reproducibility\nseed = 42\ntorch.manual_seed(seed)\nnp.random.seed(seed)\ntorch.cuda.manual_seed_all(seed)\ntorch.backends.cudnn.deterministic = True\ntorch.backends.cudnn.benchmark = False\n\n# Set device\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(f\"Using device: {device}\")\n\n# Load the California housing dataset (first 100 samples for simplicity)\ncalifornia = fetch_california_housing()\nX, y = california.data[:100], california.target[:100]\n\n# Split into train/test sets\nX_train, X_test, y_train, y_test = train_test_split(\n    X, y, test_size=0.2, random_state=seed\n)\n\n# Standardize features\nscaler = StandardScaler()\nX_train = scaler.fit_transform(X_train)\nX_test = scaler.transform(X_test)\n\n# Convert data to PyTorch tensors and move to device\nX_train_tensor = torch.from_numpy(X_train).float().to(device)\nX_test_tensor = torch.from_numpy(X_test).float().to(device)\ny_train_tensor = torch.from_numpy(y_train).float().unsqueeze(1).to(device)\ny_test_tensor = torch.from_numpy(y_test).float().unsqueeze(1).to(device)\n\n# Create a DataLoader for batching\ndataset = torch.utils.data.TensorDataset(X_train_tensor, y_train_tensor)\ndataloader = torch.utils.data.DataLoader(dataset, batch_size=20, shuffle=True)\n\nUsing device: cpu\n\n\n\n\nTraining the Network\nNow, let’s define the training function and train our neural network.\n\n# Train the neural network model\ninput_size = X_train.shape[1]\nmodel = NeuralNetwork(input_size)\nloss_fn = nn.MSELoss()\noptimizer = optim.Adam(model.parameters(), lr=0.01)\nn_epochs = 100\n\nfor epoch in range(n_epochs):\n    model.train()\n    for batch_X, batch_y in dataloader:\n        # step 1: make prediction (forward pass)\n        outputs = model(batch_X)\n        # step 2: compute loss\n        loss = loss_fn(outputs, batch_y)\n        # step 3: compute gradients (backpropagation)\n        loss.backward()\n        # step 4: update parameters\n        optimizer.step()\n        # zero gradients\n        optimizer.zero_grad()\n\n\n\nEvaluating the trained Network\nAfter training, we need to evaluate our network’s performance on the test set.\n\n# Evaluate the model\nmodel.eval()\nwith torch.no_grad():\n    y_pred_nn = model(X_test_tensor).numpy()\n\nmse_nn = mean_squared_error(y_test, y_pred_nn)\nr2_nn = r2_score(y_test, y_pred_nn)\n\nprint(f'MSE (Neural Network): {mse_nn}')\nprint(f'R2 (Neural Network): {r2_nn}')\n\nMSE (Neural Network): 0.143800164172336\nR2 (Neural Network): 0.8298946833143164\n\n\n\n\nConclusion\nIn this blog post, we covered the basics of building a neural network using PyTorch. We went through setting up the environment, defining the neural network, preparing the data, training the network, and evaluating its performance. PyTorch provides a flexible and intuitive framework for developing deep learning models, making it a popular choice for researchers and practitioners alike.\nFeel free to experiment with different architectures, hyperparameters, and datasets to further enhance your understanding and build more complex models.\n\n\nOnline Resources on PyTorch and Deep Learning\n\nPyTorch Documentation\nDeep Learning with PyTorch: A 60 Minute Blitz\nLearning PyTorch with Examples\nLearn PyTorch for Deep Learning\nDeep Learning Book\nUnderstanding Deep Learning\nThe Little Book of Deep Learning\nFast.ai Course\nDive into Deep Learning\nStanford CS230: Deep Learning\nStanford CS231n: Convolutional Neural Networks for Visual Recognition\nMIT Deep Learning course\nUC Berkeley CS182\nExplain Backpropagation\nVisualizing machine learning\nPlayground for tensorflow"
  },
  {
    "objectID": "posts/blog_efficient_compute.html",
    "href": "posts/blog_efficient_compute.html",
    "title": "Accelerating Your Python Code: Tips for Faster Performance",
    "section": "",
    "text": "In this blog post, I will explore several techniques to accelerate your Python code and enhance its performance."
  },
  {
    "objectID": "posts/blog_efficient_compute.html#use-broadcasting-to-avoid-creating-a-diagonal-matrix",
    "href": "posts/blog_efficient_compute.html#use-broadcasting-to-avoid-creating-a-diagonal-matrix",
    "title": "Accelerating Your Python Code: Tips for Faster Performance",
    "section": "Use broadcasting to avoid creating a diagonal matrix",
    "text": "Use broadcasting to avoid creating a diagonal matrix\n\nUse A * v instead of A @ np.diag(v)\nUse v[:, np.newaxis] * A instead of np.diag(v) @ A\n\n\nimport numpy as np\nimport timeit\nA = np.random.randn(1000, 1000)\nv = np.random.randn(1000)\nassert np.all( A @ np.diag(v) == A * v )\nassert np.all( np.diag(v) @ A == v[:, np.newaxis] * A )\nprint('time for method 1:', timeit.timeit(lambda: A @ np.diag(v), number=10))\nprint('time for method 2:', timeit.timeit(lambda: A * v, number=10))\n\ntime for method 1: 0.2450430829776451\ntime for method 2: 0.013035333948209882"
  },
  {
    "objectID": "posts/blog_efficient_compute.html#avoid-large-matrix-multiplication",
    "href": "posts/blog_efficient_compute.html#avoid-large-matrix-multiplication",
    "title": "Accelerating Your Python Code: Tips for Faster Performance",
    "section": "Avoid large matrix multiplication",
    "text": "Avoid large matrix multiplication\n\nUse np.einsum('ij,ji-&gt;', A, B) instead of np.trace(A @ B)\n\n\nimport numpy as np\nimport timeit\nA = np.random.randn(1000, 1000)\nB = np.random.randn(1000, 1000)\nassert np.isclose(np.trace(A @ B), np.einsum('ij,ji-&gt;', A, B))\nprint('time for method 1:', timeit.timeit(lambda: np.trace(A @ B), number=10))\nprint('time for method 2:', timeit.timeit(lambda: np.einsum('ij,ji-&gt;', A, B), number=10))\n\ntime for method 1: 0.2524136659922078\ntime for method 2: 0.010554542066529393"
  },
  {
    "objectID": "posts/blog_efficient_compute.html#prioritize-the-order-of-matrix-multiplication",
    "href": "posts/blog_efficient_compute.html#prioritize-the-order-of-matrix-multiplication",
    "title": "Accelerating Your Python Code: Tips for Faster Performance",
    "section": "Prioritize the order of matrix multiplication",
    "text": "Prioritize the order of matrix multiplication\n\nUse A @ (B @ v) instead of A @ B @ v when v is a vector or an array of smaller size.\n\n\nimport numpy as np\nimport timeit\nA = np.random.randn(1000, 1000)\nB = np.random.randn(1000, 1000)\nv = np.random.randn(1000)\nassert np.allclose(A @ B @ v, A @ (B @ v))\nprint('time for method 1:', timeit.timeit(lambda: A @ B @ v, number=10))\nprint('time for method 2:', timeit.timeit(lambda: A @ (B @ v), number=10))\n\ntime for method 1: 0.29409229196608067\ntime for method 2: 0.002889833995141089"
  },
  {
    "objectID": "teaching.html",
    "href": "teaching.html",
    "title": "Teaching",
    "section": "",
    "text": "Term\nCourse Title\nCourse Level\n\n\n\n\nSummer 2022\nProbability and Stochastic Processes\nFirst-year PhD\n\n\nSpring 2020\nStatistical Methods and Motivations\nUndergraduate\n\n\nFall 2019\nStatistical Methods and Motivations\nUndergraduate\n\n\n\n\n\n\n\n\n\n\n\n\n\nTerm\nCourse Title\nCourse Level\n\n\n\n\nFall 2024\nSTAT 534 Stat Learning for Data Science\nMaster\n\n\nSpring 2024\nSTAT 597 Data Wrangling\nMaster\n\n\nFall 2022\nSTAT 592 Theory of Probability\nPhD\n\n\nSpring 2021\nSTAT 401 Basic Statistics For Research\nUndergraduate\n\n\nSpring 2021\nSTAT 211 Statistics I\nUndergraduate\n\n\nFall 2020\nSTAT 401 Basic Statistics For Research\nUndergraduate\n\n\nFall 2020\nSTAT 211 Statistics I\nUndergraduate\n\n\nSpring 2019\nSTA 296 Statistical Methods and Motivations\nUndergraduate\n\n\nFall 2018\nSTA 296 Statistical Methods and Motivations\nUndergraduate"
  },
  {
    "objectID": "teaching.html#independent-instructor",
    "href": "teaching.html#independent-instructor",
    "title": "Teaching",
    "section": "",
    "text": "Term\nCourse Title\nCourse Level\n\n\n\n\nSummer 2022\nProbability and Stochastic Processes\nFirst-year PhD\n\n\nSpring 2020\nStatistical Methods and Motivations\nUndergraduate\n\n\nFall 2019\nStatistical Methods and Motivations\nUndergraduate\n\n\n\n\n\n\n\n\n\n\n\n\n\nTerm\nCourse Title\nCourse Level\n\n\n\n\nFall 2024\nSTAT 534 Stat Learning for Data Science\nMaster\n\n\nSpring 2024\nSTAT 597 Data Wrangling\nMaster\n\n\nFall 2022\nSTAT 592 Theory of Probability\nPhD\n\n\nSpring 2021\nSTAT 401 Basic Statistics For Research\nUndergraduate\n\n\nSpring 2021\nSTAT 211 Statistics I\nUndergraduate\n\n\nFall 2020\nSTAT 401 Basic Statistics For Research\nUndergraduate\n\n\nFall 2020\nSTAT 211 Statistics I\nUndergraduate\n\n\nSpring 2019\nSTA 296 Statistical Methods and Motivations\nUndergraduate\n\n\nFall 2018\nSTA 296 Statistical Methods and Motivations\nUndergraduate"
  },
  {
    "objectID": "research.html",
    "href": "research.html",
    "title": "Research",
    "section": "",
    "text": "Uncertainty quantification for iterative algorithms in linear models with application to early stopping\nPierre C. Bellec, Kai Tan (2024) (alphabetical order)\nMajor revision at the Annals of Statistics.\nPrecise Asymptotics of Bagging Regularized M-estimators\nTakuya Koriyama, Pratik Patil, Jin-Hong Du, Kai Tan, Pierre C. Bellec (2024)\nSubmitted to the Annals of Statistics.\nFréchet sufficient variable selection with graphical structure among predictors.\nJiaying Weng \\(^*\\) , Kai Tan \\(^*\\) (\\(^*\\)=equal contribution), Cheng Wang and Zhou Yu (2023)\nMajor revision at the Scandinavian Journal of Statistics."
  },
  {
    "objectID": "research.html#preprints",
    "href": "research.html#preprints",
    "title": "Research",
    "section": "",
    "text": "Uncertainty quantification for iterative algorithms in linear models with application to early stopping\nPierre C. Bellec, Kai Tan (2024) (alphabetical order)\nMajor revision at the Annals of Statistics.\nPrecise Asymptotics of Bagging Regularized M-estimators\nTakuya Koriyama, Pratik Patil, Jin-Hong Du, Kai Tan, Pierre C. Bellec (2024)\nSubmitted to the Annals of Statistics.\nFréchet sufficient variable selection with graphical structure among predictors.\nJiaying Weng \\(^*\\) , Kai Tan \\(^*\\) (\\(^*\\)=equal contribution), Cheng Wang and Zhou Yu (2023)\nMajor revision at the Scandinavian Journal of Statistics."
  },
  {
    "objectID": "research.html#publications",
    "href": "research.html#publications",
    "title": "Research",
    "section": "Publications",
    "text": "Publications\n\nEstimating Generalization Performance Along the Trajectory of Proximal SGD in Robust Regression\nKai Tan, Pierre C. Bellec (2024)\nAdvances in Neural Information Processing Systems (NeurIPS 2024). [arXiv][Code]\nCorrected generalized cross-validation for finite ensembles of penalized estimators\nPierre C. Bellec, Jin-Hong Du, Takuya Koriyama, Pratik Patil, Kai Tan (2023) (alphabetical order).\nJournal of the Royal Statistical Society Series B, to appear. [arXiv] [Code]\nMultinomial Logistic Regression: Asymptotic Normality on Null Covariates in High-Dimensions\nKai Tan, and Pierre C. Bellec (2023).\nAdvances in Neural Information Processing Systems 36 (2024). [arXiv] [Code]\nNoise covariance estimation in multi-task high-dimensional linear models\nKai Tan, Gabriel Romon, and Pierre C. Bellec (2023).\nBernoulli. 30.3 (2024): 1695-1722. [arXiv] [slides] [Code]\nVariable dependent partial dimension reduction\nLu Li, Kai Tan, Xuerong Meggie Wen, and Zhou Yu (2023) (alphabetical order).\nTEST. 32, 521–541 (2023).\nSparse SIR: optimal rates and adaptive estimation\nKai Tan, Lei Shi, and Zhou Yu (2020).\nThe Annals of Statistics 48.1 (2020): 64-85.\nCOM-negative binomial distribution: modeling overdispersion and ultrahigh zero-inflated count data\nHuiming Zhang, Kai Tan and Bo Li (2018).\nFrontiers of Mathematics in China, 13(4), 967-998.\nSummaries of three keynote lectures at the SAE-2018\nKai Tan, and Lyu Ni (2018).\nStatistical Theory and Related Fields 2.2 (2018): 215-218.\nInterview with Professor Danny Pfeffermann\nLyu Ni, and Kai Tan (2018).\nStatistical Theory and Related Fields 2.2 (2018): 219-221."
  },
  {
    "objectID": "posts/running-jupyter-amarel.html",
    "href": "posts/running-jupyter-amarel.html",
    "title": "Running Jupyter Notebooks on Amarel’s GPU Nodes",
    "section": "",
    "text": "This tutorial provides a step-by-step guide on running a Jupyter Notebook on Amarel’s GPU nodes at Rutgers. Follow the instructions below to set up your Jupyter environment and access it from your local machine."
  },
  {
    "objectID": "posts/running-jupyter-amarel.html#overview",
    "href": "posts/running-jupyter-amarel.html#overview",
    "title": "Running Jupyter Notebooks on Amarel’s GPU Nodes",
    "section": "",
    "text": "This tutorial provides a step-by-step guide on running a Jupyter Notebook on Amarel’s GPU nodes at Rutgers. Follow the instructions below to set up your Jupyter environment and access it from your local machine."
  },
  {
    "objectID": "posts/running-jupyter-amarel.html#prerequisites",
    "href": "posts/running-jupyter-amarel.html#prerequisites",
    "title": "Running Jupyter Notebooks on Amarel’s GPU Nodes",
    "section": "Prerequisites",
    "text": "Prerequisites\n\nYou need access to the Rutgers Amarel cluster, you can request access at OARC website.\nYou need to use VPN to connect to the Rutgers network if you are off-campus."
  },
  {
    "objectID": "posts/running-jupyter-amarel.html#step-1-ssh-into-amarel",
    "href": "posts/running-jupyter-amarel.html#step-1-ssh-into-amarel",
    "title": "Running Jupyter Notebooks on Amarel’s GPU Nodes",
    "section": "Step 1: SSH into Amarel",
    "text": "Step 1: SSH into Amarel\nStart by opening your terminal and connecting to Amarel using SSH. Replace &lt;netid&gt; with your actual NetID:\nssh &lt;netid&gt;@amarel.rutgers.edu"
  },
  {
    "objectID": "posts/running-jupyter-amarel.html#step-2-start-an-interactive-session-on-a-gpu-node",
    "href": "posts/running-jupyter-amarel.html#step-2-start-an-interactive-session-on-a-gpu-node",
    "title": "Running Jupyter Notebooks on Amarel’s GPU Nodes",
    "section": "Step 2: Start an Interactive Session on a GPU Node",
    "text": "Step 2: Start an Interactive Session on a GPU Node\nOnce logged into Amarel, initiate an interactive session on a GPU node by running:\nsrun -N 1 --gres=gpu:1 --partition=gpu --mem=4G --time=02:00:00 --constraint=ampere --pty bash\n\nExplanation:\n\n-N 1: Requests 1 node.\n--gres=gpu:1: Allocates 1 GPU.\n--partition=gpu: Specifies the GPU partition.\n--mem=4G: Requests 4GB of memory.\n--time=02:00:00: Allocates 2 hours for the session.\n--constraint=ampere: Requests an Ampere GPU (e.g., NVIDIA A100).\n--pty bash: Opens a bash shell in the allocated session. This command will allocate the requested resources and start an interactive session."
  },
  {
    "objectID": "posts/running-jupyter-amarel.html#step-3-start-jupyter-notebook-server",
    "href": "posts/running-jupyter-amarel.html#step-3-start-jupyter-notebook-server",
    "title": "Running Jupyter Notebooks on Amarel’s GPU Nodes",
    "section": "Step 3: Start Jupyter Notebook Server",
    "text": "Step 3: Start Jupyter Notebook Server\nEnsure that Jupyter Notebook is installed in your Python environment on Amarel. You can use the community available Anaconda distribution by\nmodule use /projects/community/modulefiles\nmodule load anaconda/2024.06-ts840\nStart the Jupyter Notebook server with:\njupyter notebook --no-browser --ip=0.0.0.0 --port=8889\nIn the prompted message, note the node on which your job is running (e.g., gpu123)."
  },
  {
    "objectID": "posts/running-jupyter-amarel.html#step-4-set-up-ssh-port-forwarding",
    "href": "posts/running-jupyter-amarel.html#step-4-set-up-ssh-port-forwarding",
    "title": "Running Jupyter Notebooks on Amarel’s GPU Nodes",
    "section": "Step 4: Set Up SSH Port Forwarding",
    "text": "Step 4: Set Up SSH Port Forwarding\nTo connect your local machine to the Jupyter Notebook server running on Amarel, open a NEW terminal and set up an SSH tunnel:\nssh -L 9000:&lt;node_name&gt;:8889 &lt;netid&gt;@amarel.hpc.rutgers.edu\n\nReplace &lt;node_name&gt; with the node name from the interactive session (e.g., gpu123).\nReplace &lt;netid&gt; with your actual NetID.\n\n\nExplanation:\n\n-L 9000:&lt;node_name&gt;:8889: Forwards the port 8889 from Amarel’s compute node to localhost:9000 on your local machine."
  },
  {
    "objectID": "posts/running-jupyter-amarel.html#step-5-access-jupyter-notebook-from-your-browser",
    "href": "posts/running-jupyter-amarel.html#step-5-access-jupyter-notebook-from-your-browser",
    "title": "Running Jupyter Notebooks on Amarel’s GPU Nodes",
    "section": "Step 5: Access Jupyter Notebook from Your Browser",
    "text": "Step 5: Access Jupyter Notebook from Your Browser\nOpen your web browser and enter:\nlocalhost:9000\nThis will open the Jupyter Notebook interface that is running on Amarel’s compute node.\n\nIf you are prompted for a token, copy the token from the terminal where your Jupyter Notebook is running.\nYou can now create or upload notebooks and run them on the GPU resources provided."
  },
  {
    "objectID": "posts/running-jupyter-amarel.html#acknowledgments",
    "href": "posts/running-jupyter-amarel.html#acknowledgments",
    "title": "Running Jupyter Notebooks on Amarel’s GPU Nodes",
    "section": "Acknowledgments",
    "text": "Acknowledgments\nThis tutorial was adapted from the guidance provided by Robert Palmere, Senior Scientist at the Office of Advanced Research Computing (OARC) at Rutgers University. I greatly appreciate the help from Robert."
  },
  {
    "objectID": "posts/PyTroch_Simple_Linear_Regression.html",
    "href": "posts/PyTroch_Simple_Linear_Regression.html",
    "title": "Learning PyTorch: Simple Linear Regression",
    "section": "",
    "text": "In this post, I will demonstrate how to implement a simple linear regression model using PyTorch."
  },
  {
    "objectID": "posts/PyTroch_Simple_Linear_Regression.html#solve-linear-regression-by-gradient-descent-in-numpy",
    "href": "posts/PyTroch_Simple_Linear_Regression.html#solve-linear-regression-by-gradient-descent-in-numpy",
    "title": "Learning PyTorch: Simple Linear Regression",
    "section": "Solve Linear Regression by Gradient Descent in NumPy",
    "text": "Solve Linear Regression by Gradient Descent in NumPy\n\nStep 1: Given weight \\(w\\) and bias \\(b\\), compute prediction (Forward pass): \\[\\hat{y}_{i} = b + w x_{i}\\]\nStep 2: compute the loss: \\[\nL(w,b)\n= \\frac1n \\sum_{i=1}^n (y_i - \\hat y_i)^2\n= \\frac1n \\sum_{i=1}^n (y_i - b - w * x_i)^2\n\\]\nStep 3: compute the gradients (backward pass): \\[\n\\frac{\\partial L}{\\partial w}\n= -\\frac{2}{n} \\sum_{i=1}^n (y_i - b - w x_i) x_i\n= -\\frac{2}{n} \\sum_{i=1}^n (y_i - \\hat y_i) x_i\n\\] \\[\n\\frac{\\partial L}{\\partial b}\n= -\\frac{2}{n} \\sum_{i=1}^n (y_i - b - w x_i)\n= -\\frac{2}{n} \\sum_{i=1}^n (y_i - \\hat y_i)\n\\]\nStep 4: update the parameters: \\[\nw \\leftarrow w - \\eta \\frac{\\partial L}{\\partial w}\\] \\[\nb \\leftarrow b - \\eta \\frac{\\partial L}{\\partial b}\n\\]\n\n\n# initialization of weight and bias\nw = rng.standard_normal(1)\nb = rng.standard_normal(1)\nprint('Initial bias:', b)\nprint('Initial weight:', w)\n\nlr = 0.01 # learning rate\nn_epochs = 1000 # number of iterations\nfor t in range(n_epochs):\n    # step 1: prediction\n    y_pred = b + w * x\n    # step 2: compute loss\n    loss = np.mean((y - y_pred) ** 2)\n    # step 3: compute gradients\n    dw = -2 * np.mean(x * (y - y_pred))\n    db = -2 * np.mean(y - y_pred)\n    # step4: update weights and bias\n    w -= lr * dw\n    b -= lr * db\nprint('Final bias:', b)\nprint('Final weight:', w)\n\n# plot the results\nplt.plot(x, y, 'b.')\nplt.plot(x, b + x * w, 'r-')\nplt.xlabel('x')\nplt.ylabel('y')\nplt.title('Linear Regression using Gradient Descent')\nplt.show()\n\nInitial bias: [-0.14451867]\nInitial weight: [-0.67517827]\nFinal bias: [1.00716318]\nFinal weight: [1.99588476]"
  },
  {
    "objectID": "posts/PyTroch_Simple_Linear_Regression.html#solve-linear-regression-with-pytorch-autograd",
    "href": "posts/PyTroch_Simple_Linear_Regression.html#solve-linear-regression-with-pytorch-autograd",
    "title": "Learning PyTorch: Simple Linear Regression",
    "section": "Solve Linear Regression with PyTorch (autograd)",
    "text": "Solve Linear Regression with PyTorch (autograd)\n\n# using pytorch\nimport torch\nimport torch.optim as optim\nimport torch.nn as nn\n\ntorch.manual_seed(42)  # for reproducibility\n\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n# convert data to torch tensors\nx_tensor = torch.from_numpy(x).float().to(device)\ny_tensor = torch.from_numpy(y).float().to(device)\n\nlr = 0.1\nn_epochs = 1000\n# initial weights and bias\nb = torch.randn(1, requires_grad=True, device=device)\nw = torch.randn(1, requires_grad=True, device=device)\n\nfor t in range(n_epochs):\n    # step 1: prediction\n    y_hat = b + w * x_tensor\n    # step 2: compute loss\n    error = y_tensor - y_hat\n    loss = torch.mean(error ** 2)\n    # step 3: compute gradients (via autograd)\n    loss.backward()\n    # step 4: update weights and bias\n    with torch.no_grad():\n        b -= lr * b.grad\n        w -= lr * w.grad\n    b.grad.zero_()\n    w.grad.zero_()\nprint('Final bias (PyTorch):', b)\nprint('Final weight (PyTorch):', w)\n\nFinal bias (PyTorch): tensor([1.0072], requires_grad=True)\nFinal weight (PyTorch): tensor([1.9959], requires_grad=True)"
  },
  {
    "objectID": "posts/PyTroch_Simple_Linear_Regression.html#solve-linear-regression-with-pytorch-autograd-loss-optimizer",
    "href": "posts/PyTroch_Simple_Linear_Regression.html#solve-linear-regression-with-pytorch-autograd-loss-optimizer",
    "title": "Learning PyTorch: Simple Linear Regression",
    "section": "Solve Linear Regression with PyTorch (autograd + loss + optimizer)",
    "text": "Solve Linear Regression with PyTorch (autograd + loss + optimizer)\n\nb = torch.randn(1, requires_grad=True, dtype=torch.float, device=device)\nw = torch.randn(1, requires_grad=True, dtype=torch.float, device=device)\n\nlr = 0.01\nn_epochs = 1000\nloss_fn = nn.MSELoss(reduction='mean')\noptimizer = optim.SGD([b, w], lr=lr)\nfor t in range(n_epochs):\n    # step 1: prediction\n    y_hat = b + w * x_tensor\n    # step 2: compute loss\n    loss = loss_fn(y_tensor, y_hat)\n    # step 3: clear old and compute current gradients\n    loss.backward() # compute gradients\n    # step 4: update weights and bias using the specified optimizer (e.g. SGD)\n    optimizer.step()\n    # clear gradients for the next iteration\n    optimizer.zero_grad()\nprint('Final bias (PyTorch with optim):', b)\nprint('Final weight (PyTorch with optim):', w)\n\nFinal bias (PyTorch with optim): tensor([1.0072], requires_grad=True)\nFinal weight (PyTorch with optim): tensor([1.9959], requires_grad=True)"
  },
  {
    "objectID": "posts/PyTroch_Simple_Linear_Regression.html#object-oriented-programming",
    "href": "posts/PyTroch_Simple_Linear_Regression.html#object-oriented-programming",
    "title": "Learning PyTorch: Simple Linear Regression",
    "section": "Object-oriented programming",
    "text": "Object-oriented programming\n\nimport torch\nimport torch.nn as nn\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\nclass LinearRegression(nn.Module):\n    def __init__(self):\n        super().__init__()\n        # Initialize parameters\n        self.b = nn.Parameter(torch.randn(1, device=device)) \n        self.w = nn.Parameter(torch.randn(1, device=device)) \n\n    def forward(self, x):\n        # Forward pass: compute predicted y\n        return self.b + self.w * x\n\n# Create model\nmodel = LinearRegression().to(device)\n# define the loss function and optimizer\nlr = 0.01\nn_epochs = 1000\nloss_fn = nn.MSELoss(reduction='mean')\noptimizer = optim.SGD(model.parameters(), lr=lr)\n\nfor t in range(n_epochs):\n    model.train()\n    # step 1: makd prediction (forward pass)\n    y_hat = model(x_tensor)\n    # step 2: compute loss\n    loss = loss_fn(y_tensor, y_hat)\n    # step 3: compute gradients (via autograd)\n    loss.backward()\n    # step 4: update parameters via an optimizer (e.g. SGD)\n    optimizer.step() \n    # clear gradients for the next iteration\n    optimizer.zero_grad()\n\nprint(model.state_dict())\nprint('Final bias (class + PyTorch):', model.b.item())\nprint('Final weight (class + PyTorch):', model.w.item())\n\nOrderedDict({'b': tensor([1.0072]), 'w': tensor([1.9959])})\nFinal bias (class + PyTorch): 1.0071604251861572\nFinal weight (class + PyTorch): 1.9958817958831787"
  },
  {
    "objectID": "posts/PyTroch_Simple_Linear_Regression.html#create-linear-model-via-nn.sequential",
    "href": "posts/PyTroch_Simple_Linear_Regression.html#create-linear-model-via-nn.sequential",
    "title": "Learning PyTorch: Simple Linear Regression",
    "section": "Create Linear Model via nn.Sequential",
    "text": "Create Linear Model via nn.Sequential\n\nlr = 0.01\nn_epochs = 1000\nmodel = nn.Sequential(nn.Linear(1, 1)).to(device)\nloss_fn = nn.MSELoss(reduction='mean')\noptimizer = optim.SGD(model.parameters(), lr=lr)\n\nlosses = []\nfor t in range(n_epochs):\n    model.train()\n    # step 1: make prediction (forward pass)\n    y_hat = model(x_tensor)\n    # step 2: compute loss\n    loss = loss_fn(y_tensor, y_hat)\n    losses.append(loss.item())\n    # step 3: compute gradients (via autograd)\n    loss.backward()\n    # step 4: update parameters via an optimizer (e.g. SGD)\n    optimizer.step() \n    # clear gradients for the next iteration\n    optimizer.zero_grad()\n\nprint(model.state_dict())\nplt.plot(losses)\nplt.xlabel('Epochs')\nplt.ylabel('Loss')\nplt.title('Training Loss over Epochs')\nplt.show()\n\nOrderedDict({'0.weight': tensor([[1.9959]]), '0.bias': tensor([1.0072])})"
  },
  {
    "objectID": "posts/PyTroch_Simple_Linear_Regression.html#dataset-and-data-loader",
    "href": "posts/PyTroch_Simple_Linear_Regression.html#dataset-and-data-loader",
    "title": "Learning PyTorch: Simple Linear Regression",
    "section": "Dataset and Data Loader",
    "text": "Dataset and Data Loader\n\nfrom torch.utils.data import Dataset, TensorDataset\nclass myDataset(Dataset):\n    def __init__(self, x, y):\n        self.x = x_tensor\n        self.y = y_tensor\n    def __getitem__(self, index):\n        return self.x[index], self.y[index]\n    \n    def __len__(self):\n        return len(self.x)\n\ntrain_data = myDataset(x_tensor, y_tensor)\nprint('Length of train_data:', len(train_data))\nprint('First item in train_data:', train_data[0])\n\ntrain_data1 = TensorDataset(x_tensor, y_tensor)\nprint('Length of train_data1:', len(train_data1))\nprint('First item in train_data1:', train_data1[0])\n\nLength of train_data: 1000\nFirst item in train_data: (tensor([0.4967]), tensor([2.1334]))\nLength of train_data1: 1000\nFirst item in train_data1: (tensor([0.4967]), tensor([2.1334]))"
  },
  {
    "objectID": "posts/PyTroch_Simple_Linear_Regression.html#solve-linear-regression-with-dataset-and-data-loader",
    "href": "posts/PyTroch_Simple_Linear_Regression.html#solve-linear-regression-with-dataset-and-data-loader",
    "title": "Learning PyTorch: Simple Linear Regression",
    "section": "Solve Linear Regression with Dataset and Data Loader",
    "text": "Solve Linear Regression with Dataset and Data Loader\n\nfrom torch.utils.data import DataLoader\ntrain_loader = DataLoader(train_data, batch_size=50, shuffle=True)\nlr = 0.01\nn_epochs = 1000\nmodel = nn.Sequential(nn.Linear(1, 1)).to(device)\nloss_fn = nn.MSELoss(reduction='mean')\noptimizer = optim.SGD(model.parameters(), lr=lr)\nfor t in range(n_epochs):\n    for x_batch, y_batch in train_loader:\n        x_batch = x_batch.to(device)\n        y_batch = y_batch.to(device)\n        model.train()\n        # step 1: make prediction (forward pass)\n        y_hat = model(x_batch)\n        # step 2: compute loss\n        loss = loss_fn(y_batch, y_hat)\n        losses.append(loss.item())\n        # step 3: compute gradients (via autograd)\n        loss.backward()\n        # step 4: update parameters via an optimizer (e.g. SGD)\n        optimizer.step() \n        # clear gradients for the next iteration\n        optimizer.zero_grad()        \nprint(model.state_dict())\n\nOrderedDict({'0.weight': tensor([[1.9958]]), '0.bias': tensor([1.0073])})"
  },
  {
    "objectID": "posts/PyTroch_Simple_Linear_Regression.html#evaluation-in-validation-set",
    "href": "posts/PyTroch_Simple_Linear_Regression.html#evaluation-in-validation-set",
    "title": "Learning PyTorch: Simple Linear Regression",
    "section": "Evaluation in validation set",
    "text": "Evaluation in validation set\n\nfrom torch.utils.data import random_split\ndataset = TensorDataset(x_tensor, y_tensor)\ntrain_dataset, val_dataset = random_split(dataset, [800, 200])\ntrain_loader = DataLoader(train_dataset, batch_size=40)\nval_loader = DataLoader(val_dataset, batch_size=20)\nlosses = []\nval_losses = []\n\nfor t in range(n_epochs):\n    for x_batch, y_batch in train_loader:\n        x_batch = x_batch.to(device)\n        y_batch = y_batch.to(device)\n\n        model.train()\n        # step 1: make prediction (forward pass)\n        y_hat = model(x_batch)\n        # step 2: compute loss\n        loss = loss_fn(y_batch, y_hat)\n        # step 3: compute gradients (via autograd)\n        loss.backward()\n        # step 4: update parameters via an optimizer (e.g. SGD)\n        optimizer.step() \n        # clear gradients for the next iteration\n        optimizer.zero_grad()\n        \n        # store the loss for plotting later\n        losses.append(loss)\n    \n    # validation step (no need to track gradients)\n    with torch.no_grad():\n        for x_val, y_val in val_loader:\n            model.eval()\n            # step 1: prediction\n            y_hat = model(x_val.to(device))\n            # step 2: compute loss\n            val_loss = loss_fn(y_val.to(device), y_hat)\n            val_losses.append(val_loss.item())\nprint(model.state_dict())\n\nOrderedDict({'0.weight': tensor([[1.9960]]), '0.bias': tensor([1.0072])})"
  },
  {
    "objectID": "posts/PyTroch_Simple_Linear_Regression.html#summary",
    "href": "posts/PyTroch_Simple_Linear_Regression.html#summary",
    "title": "Learning PyTorch: Simple Linear Regression",
    "section": "Summary",
    "text": "Summary\n\nNo need to manually compute gradients; PyTorch’s autograd does it for you.\nUse torch.nn for loss functions and optimizers.\nUse nn.Sequential for building models in a modular way.\nUse torch.utils.data.Dataset and DataLoader for data handling.\nUse validation sets to evaluate model performance.\nReference"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Kai Tan",
    "section": "",
    "text": "Welcome to my website! I’m a 5th-year Ph.D. student in Department of Statistics at Rutgers University. I am very fortunate to have Pierre C. Bellec as my advisor. My research interests span a wide range of areas in Statistics and Machine Learning, including high dimensional statistical inference, dimension reduction, and causal inference. Specifically, I develop tools for uncertainty quantification of iterative algorithms (e.g. estimating prediction risks and constructing confidence intervals for parameters of interest), to improve the reliability of statistical machine learning methods."
  },
  {
    "objectID": "index.html#recent-news",
    "href": "index.html#recent-news",
    "title": "Kai Tan",
    "section": "Recent news",
    "text": "Recent news\n\n[Nov. 2024]: The paper Uncertainty quantification for iterative algorithms in linear models with application to early stopping has been invited for a major revision at the Annals of Statistics.\n[Sep. 2024]: The paper Estimating Generalization Performance Along the Trajectory of Proximal SGD in Robust Regression is accepted to NeurIPS 2024.\n[Aug. 2024]: The paper Corrected generalized cross-validation for finite ensembles of penalized estimators is accepted to the Journal of the Royal Statistical Society, Series B (Statistical Methodology).\n[Jun. 2024]: Presented a poster at DIMACS Workshop on Modeling Randomness in Neural Network Training.\n[Sep. 2023]: The paper Multinomial Logistic Regression: Asymptotic Normality on Null Covariates in High-Dimensions is accepted to NeurIPS 2023.\n[May 2023]: I am honored to receive the Gold Medal of student poster competition at the Conference on Recent Advances in Statistics and Data Science, with a Celebration of Professors Regina Liu and Cun-Hui Zhang’s Special Birthdays.\n[March 2023]: I am honored to receive the IMS Hannan Graduate Student Travel Award.\n[Jan. 2023]: I am honored to receive the Travel Award at the 2023 Statistics Annual Winter Workshop titled “Modern Computational Statistics” hosted by the statistics department at the University of Florida."
  }
]