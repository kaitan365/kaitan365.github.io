{
  "hash": "b2ea30bc422afbf58930ead8d16fbb08",
  "result": {
    "engine": "jupyter",
    "markdown": "---\ntitle: \"PyTorch: Logistic Regression\"\nauthor: \"Kai Tan\"\ndate: \"2025-04-21\"\ncategories: [numpy, pytorch, tutorial]\nformat:\n  html:\n    toc: true\n    code-fold: false\njupyter: python3\n---\n\n\n## Logistic Regression Model\n\nThe logistic regression model estimates the probability of the label $y$ given the input $x$ by:\n\n$$\n\\mathbb{P}(y = 1) = \\sigma(w^\\top x + b)\n$$\n\nHere:\n\n- $x \\in \\mathbb{R}^d$ is the input vector, $y \\in \\{0, 1\\}$ is the binary label  \n- $w \\in \\mathbb{R}^d$ is the weight vector, $b \\in \\mathbb{R}$ is the bias (intercept)  \n- $\\sigma(z) = \\dfrac{1}{1 + e^{-z}} = \\dfrac{e^z}{1 + e^z}$ is the sigmoid function\n\n---\n\n## Binary Cross-Entropy Loss\n\nThe loss function we minimize is the **binary cross-entropy**:\n\n$$\nL(w, b) = -\\frac{1}{n} \\sum_{i=1}^n \\left[ y_i \\log(\\hat{y}_i) + (1 - y_i) \\log(1 - \\hat{y}_i) \\right]\n$$\n\nHere:\n\n- $n$ is the number of training examples \n- $y_i$ is the true label for the $i$-th sample \n- $\\hat{y}_i = \\sigma(w^\\top x_i + b)$ is the predicted probability \n\n### Intuition\n\n- If $y_i = \\hat{y}_i = 1$ for all $i$, the loss is 0 — perfect prediction.\n- If $y_i = \\hat{y}_i = 0$ for all $i$, the loss is also 0.\n- If $y_i = 1$, $\\hat{y}_i = 0$, the loss becomes infinite — the worst-case prediction.\n\n---\n\n## Deriving the Gradient\n\nWe start with the individual loss for each sample:\n\n$$\nL_i = -\\left[ y_i \\log(\\sigma(z_i)) + (1 - y_i) \\log(1 - \\sigma(z_i)) \n\\right],\n$$\n\nwhere $z_i = w^\\top x_i + b$. Using the identity\n$\\frac{d\\sigma(z)}{dz} = \\sigma(z)(1 - \\sigma(z))$,\nwe obtain the gradient:\n\n\\begin{align*}\n\\frac{\\partial L_i}{\\partial z_i} \n&= -\\left[ y_i \\frac{1}{\\sigma(z_i)} \\frac{\\partial \\sigma(z_i)}{\\partial z_i} + (1 - y_i) \\frac{-1}{1 - \\sigma(z_i)} \\frac{\\partial \\sigma(z_i)}{\\partial z_i} \\right]\\\\\n&= -\\left[ y_i \\bigl(1 - \\sigma(z_i)\\bigr) - (1 - y_i) \\sigma(z_i) \\right]\\\\\n&= \\sigma(z_i) - y_i\\\\\n&= \\hat{y}_i - y_i.\n\\end{align*}\n\n\nBy the chain rule, we can compute the gradient of the loss w.r.t. the weights $w$ and bias $b$: \n\\begin{align*}\n\\frac{\\partial L_i}{\\partial w} &= \\frac{\\partial L_i}{\\partial z_i} \\cdot \\frac{\\partial z_i}{\\partial w} = (\\hat y_i - y_i) x_i,\\\\\n\\frac{\\partial L_i}{\\partial b} &= \\frac{\\partial L_i}{\\partial z_i} \\cdot \\frac{\\partial z_i}{\\partial b} = (\\hat y_i - y_i).\n\\end{align*}\n\nAveraging over all $n$ samples,the gradients become:\n\n\\begin{equation}\n\\frac{\\partial L}{\\partial w} = \\frac{1}{n} \\sum_{i=1}^n (\\hat{y}_i - y_i) x_i\n\\quad \\text{ and } \\quad \n\\frac{\\partial L}{\\partial b} = \\frac{1}{n} \\sum_{i=1}^n (\\hat{y}_i - y_i).\n\\end{equation}\n\n---\n\n## Step 1: Data Generation\n\n::: {#224c114a .cell execution_count=1}\n``` {.python .cell-code}\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nnp.random.seed(0)\nn = 1000\nX = np.random.randn(n, 2)\ntrue_w = np.array([2.0, -3.0])\nbias = 0.5\n\nlogits = X @ true_w + bias\nprobs = 1 / (1 + np.exp(-logits))\ny = (probs > 0.5).astype(np.float32)\n\nplt.scatter(X[:, 0], X[:, 1], c=y, cmap='bwr', edgecolor='k')\nplt.title(\"Synthetic Binary Classification Data\")\nplt.xlabel(\"x1\")\nplt.ylabel(\"x2\")\nplt.grid(True)\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![](Pytorch_Logistic_Regression_files/figure-html/cell-2-output-1.png){width=587 height=449}\n:::\n:::\n\n\n---\n\n## Step 2: Logistic Regression with NumPy\n\n::: {#79b33d45 .cell execution_count=2}\n``` {.python .cell-code}\ndef sigmoid(z):\n    return 1 / (1 + np.exp(-z))\n\ndef binary_cross_entropy(y_true, y_pred):\n    eps = 1e-7\n    return -np.mean(y_true * np.log(y_pred + eps) + (1 - y_true) * np.log(1 - y_pred + eps))\n\nw = np.zeros(2)\nb = 0.0\nlr = 0.1\n\nfor epoch in range(100):\n    z = X @ w + b\n    y_pred = sigmoid(z)\n    loss = binary_cross_entropy(y, y_pred)\n\n    dz = y_pred - y\n    dw = X.T @ dz / n\n    db = np.mean(dz)\n\n    w -= lr * dw\n    b -= lr * db\n\n    if epoch % 10 == 0:\n        print(f\"Epoch {epoch}, Loss: {loss:.4f}\")\nprint(\"==========================\")\nprint(f\"Final weights: {w}, bias: {b}\")\n# Final parameters: w, b\nx_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1\nx_vals = np.linspace(x_min, x_max, 100)\n\n# Solve for x2 on the decision boundary line: w1*x1 + w2*x2 + b = 0\n# => x2 = -(w1*x1 + b)/w2\ny_vals = -(w[0] * x_vals + b) / w[1]\n\n# Plot\nplt.figure(figsize=(6, 4))\nplt.scatter(X[:, 0], X[:, 1], c=y, cmap='bwr', edgecolor='k')\nplt.plot(x_vals, y_vals, 'k--', label='Decision Boundary')\nplt.title(\"Logistic Regression Decision Boundary\")\nplt.xlabel(\"x1\")\nplt.ylabel(\"x2\")\nplt.legend()\nplt.grid(True)\nplt.show()\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nEpoch 0, Loss: 0.6931\nEpoch 10, Loss: 0.5710\nEpoch 20, Loss: 0.4932\nEpoch 30, Loss: 0.4403\nEpoch 40, Loss: 0.4019\nEpoch 50, Loss: 0.3726\nEpoch 60, Loss: 0.3495\nEpoch 70, Loss: 0.3305\nEpoch 80, Loss: 0.3147\nEpoch 90, Loss: 0.3013\n==========================\nFinal weights: [ 1.05673617 -1.50856125], bias: 0.3215972984185849\n```\n:::\n\n::: {.cell-output .cell-output-display}\n![](Pytorch_Logistic_Regression_files/figure-html/cell-3-output-2.png){width=513 height=376}\n:::\n:::\n\n\n---\n\n## Step 3: Logistic Regression with PyTorch\n\n::: {#15a3ae7c .cell execution_count=3}\n``` {.python .cell-code}\nimport torch\nfrom torch import nn\n\nX_tensor = torch.tensor(X, dtype=torch.float32)\ny_tensor = torch.tensor(y, dtype=torch.float32).view(-1, 1)\n\nmodel = nn.Sequential(\n    nn.Linear(2, 1),\n    nn.Sigmoid()\n)\n\nloss_fn = nn.BCELoss()\noptimizer = torch.optim.SGD(model.parameters(), lr=0.1)\n\nfor epoch in range(100):\n    # step 1: make prediction (forward pass)\n    y_pred = model(X_tensor)\n    # step 2: compute loss\n    loss = loss_fn(y_pred, y_tensor)\n    # step 3: clear old and compute current gradients\n    optimizer.zero_grad() # clear old gradients\n    loss.backward()       # compute current gradients\n    # step 4: update parameters\n    optimizer.step()\n\n    if epoch % 10 == 0:\n        print(f\"Epoch {epoch}, Loss: {loss.item():.4f}\")\n\nmodel_params = list(model.parameters())\nprint(\"==========================\")\nprint(f\"Final weights: {model_params[0].detach().numpy().flatten()}, bias: {model_params[1].item()}\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nEpoch 0, Loss: 0.4559\nEpoch 10, Loss: 0.4123\nEpoch 20, Loss: 0.3799\nEpoch 30, Loss: 0.3547\nEpoch 40, Loss: 0.3345\nEpoch 50, Loss: 0.3179\nEpoch 60, Loss: 0.3038\nEpoch 70, Loss: 0.2917\nEpoch 80, Loss: 0.2812\nEpoch 90, Loss: 0.2719\n==========================\nFinal weights: [ 1.2496184 -1.6982503], bias: 0.3892728388309479\n```\n:::\n:::\n\n\n",
    "supporting": [
      "Pytorch_Logistic_Regression_files"
    ],
    "filters": [],
    "includes": {}
  }
}