{
  "hash": "ec1804edcfc34cfaa66fbdf5463d6e5d",
  "result": {
    "engine": "jupyter",
    "markdown": "---\ntitle: \"Learning PyTorch: Neural Networks\"\nauthor: \"Kai Tan\"\ndate: \"2024-05-01\"\ncategories: [Python]\nformat:\n  html:\n    code-fold: false\njupyter: python3\n---\n\n\n# Introduction\n\nIn this blog post, we will explore how to build a neural network using PyTorch. PyTorch is a powerful and flexible deep learning framework that makes it easy to define, train, and evaluate neural networks. We will cover the following steps:\n\n1. Setting up the environment\n2. Defining the neural network\n3. Preparing the data\n4. Training the network\n5. Evaluating the network\n\n# Setting Up the Environment\n\nFirst, let's ensure that we have PyTorch installed. You can install PyTorch using pip:\n\n```{.sh}\npip install torch torchvision\n```\n\n# Defining the Neural Network\n\nNext, we will define a simple neural network with five layers. Each hidden layer will use the ReLU activation function.\n\n::: {#707c63a3 .cell execution_count=1}\n``` {.python .cell-code}\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\n\nclass NeuralNetwork(nn.Module):\n    def __init__(self, input_size):\n        super(NeuralNetwork, self).__init__()\n        self.fc1 = nn.Linear(input_size, 16)\n        self.fc2 = nn.Linear(16, 8)\n        self.fc3 = nn.Linear(8, 1)\n        \n    def forward(self, x):\n        x = torch.relu(self.fc1(x))\n        x = torch.relu(self.fc2(x))\n        x = self.fc3(x)\n        return x\n```\n:::\n\n\n# Preparing the Data\n\nFor this example, we will create a simple dataset. In a real-world scenario, you would use a dataset from a file or an online source.\n\n::: {#67787172 .cell execution_count=2}\n``` {.python .cell-code}\nimport torch\nimport numpy as np\nfrom sklearn.datasets import fetch_california_housing\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import mean_squared_error, r2_score\nfrom sklearn.preprocessing import StandardScaler\n\n# Set random seed for reproducibility\nseed = 42\ntorch.manual_seed(seed)\nnp.random.seed(seed)\ntorch.cuda.manual_seed_all(seed)\ntorch.backends.cudnn.deterministic = True\ntorch.backends.cudnn.benchmark = False\n\n# Set device\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(f\"Using device: {device}\")\n\n# Load the California housing dataset (first 100 samples for simplicity)\ncalifornia = fetch_california_housing()\nX, y = california.data[:100], california.target[:100]\n\n# Split into train/test sets\nX_train, X_test, y_train, y_test = train_test_split(\n    X, y, test_size=0.2, random_state=seed\n)\n\n# Standardize features\nscaler = StandardScaler()\nX_train = scaler.fit_transform(X_train)\nX_test = scaler.transform(X_test)\n\n# Convert data to PyTorch tensors and move to device\nX_train_tensor = torch.from_numpy(X_train).float().to(device)\nX_test_tensor = torch.from_numpy(X_test).float().to(device)\ny_train_tensor = torch.from_numpy(y_train).float().unsqueeze(1).to(device)\ny_test_tensor = torch.from_numpy(y_test).float().unsqueeze(1).to(device)\n\n# Create a DataLoader for batching\ndataset = torch.utils.data.TensorDataset(X_train_tensor, y_train_tensor)\ndataloader = torch.utils.data.DataLoader(dataset, batch_size=20, shuffle=True)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nUsing device: cpu\n```\n:::\n:::\n\n\n# Training the Network\n\nNow, let's define the training function and train our neural network.\n\n::: {#021c8cb3 .cell execution_count=3}\n``` {.python .cell-code}\n# Train the neural network model\ninput_size = X_train.shape[1]\nmodel = NeuralNetwork(input_size)\nloss_fn = nn.MSELoss()\noptimizer = optim.Adam(model.parameters(), lr=0.01)\nn_epochs = 100\n\nfor epoch in range(n_epochs):\n    model.train()\n    for batch_X, batch_y in dataloader:\n        # step 1: make prediction (forward pass)\n        outputs = model(batch_X)\n        # step 2: compute loss\n        loss = loss_fn(outputs, batch_y)\n        # step 3: clear old and compute current gradients\n        optimizer.zero_grad() # clear old gradients\n        loss.backward()       # compute current gradients\n        # step 4: update parameters\n        optimizer.step()\n```\n:::\n\n\n# Evaluating the trained Network\n\nAfter training, we need to evaluate our network's performance on the test set.\n\n::: {#762468a0 .cell execution_count=4}\n``` {.python .cell-code}\n# Evaluate the model\nmodel.eval()\nwith torch.no_grad():\n    y_pred_nn = model(X_test_tensor).numpy()\n\nmse_nn = mean_squared_error(y_test, y_pred_nn)\nr2_nn = r2_score(y_test, y_pred_nn)\n\nprint(f'MSE (Neural Network): {mse_nn}')\nprint(f'R2 (Neural Network): {r2_nn}')\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nMSE (Neural Network): 0.143800164172336\nR2 (Neural Network): 0.8298946833143164\n```\n:::\n:::\n\n\n# Conclusion\n\nIn this blog post, we covered the basics of building a neural network using PyTorch. We went through setting up the environment, defining the neural network, preparing the data, training the network, and evaluating its performance. PyTorch provides a flexible and intuitive framework for developing deep learning models, making it a popular choice for researchers and practitioners alike.\n\nFeel free to experiment with different architectures, hyperparameters, and datasets to further enhance your understanding and build more complex models.\n\n# Online Resources on PyTorch and Deep Learning\n\n- [PyTorch Documentation](https://pytorch.org/docs/stable/index.html)\n- [Deep Learning with PyTorch: A 60 Minute Blitz](https://pytorch.org/tutorials/beginner/deep_learning_60min_blitz.html)\n- [Learning PyTorch with Examples](https://docs.pytorch.org/tutorials/beginner/pytorch_with_examples.html)\n- [Learn PyTorch for Deep Learning: Zero to Mastery](https://www.learnpytorch.io)\n- [Deep Learning Book](http://www.deeplearningbook.org/)\n- [Understanding Deep Learning](https://udlbook.github.io/udlbook/)\n- [The Little Book of Deep Learning](https://fleuret.org/public/lbdl.pdf)\n- [Fast.ai Course](https://course.fast.ai/)\n- [Dive into Deep Learning](https://d2l.ai/)\n- [Stanford CS230: Deep Learning](https://cs230.stanford.edu/)\n- [Stanford CS231n: Convolutional Neural Networks for Visual Recognition](https://cs231n.stanford.edu/)\n- [MIT Deep Learning course](https://introtodeeplearning.com/)\n- [UC Berkeley CS182](https://cs182sp21.github.io/)\n- [Explain Backpropagation](https://colah.github.io/posts/2015-08-Backprop/)\n\n- [Visualizing machine learning](https://jalammar.github.io/about/)\n- [Playground for tensorflow](https://playground.tensorflow.org/)\n\n",
    "supporting": [
      "PyTorch_Neural_Networks_files"
    ],
    "filters": [],
    "includes": {}
  }
}