{
  "hash": "49bfb0a87a75778e2f773e5fdf967c5f",
  "result": {
    "engine": "jupyter",
    "markdown": "---\ntitle: \"A Practical Guide to PyTorch DataLoader\"\nauthor: \"Kai Tan\"\ndate: \"2024-04-22\"\ncategories: [PyTorch, deep learning, tutorial]\nformat:\n  html:\n    code-fold: false\njupyter: python3\n---\n\n\n\n## What Is a DataLoader?\n\nIn PyTorch, the `DataLoader` is a powerful tool that lets you iterate over datasets efficiently. It handles batching, shuffling, and even multi-process data loading â€” all crucial for model training.\n\n---\n\n## Step 1: Create a Custom Dataset\n\nStart by subclassing `torch.utils.data.Dataset`. Here's an example where we simulate binary labels.\n\n::: {#d7b9e8be .cell execution_count=1}\n``` {.python .cell-code}\nimport torch\nfrom torch.utils.data import Dataset\n\nclass MyDataset(Dataset):\n    def __init__(self):\n        self.data = torch.arange(100)\n        self.labels = self.data % 2  # binary labels\n\n    def __len__(self):\n        return len(self.data)\n\n    def __getitem__(self, idx):\n        return self.data[idx], self.labels[idx]\n```\n:::\n\n\n---\n\n## Step 2: Use DataLoader to Load in Batches\n\n::: {#73cb01fc .cell execution_count=2}\n``` {.python .cell-code}\nfrom torch.utils.data import DataLoader\n\ndataset = MyDataset()\ndataloader = DataLoader(dataset, batch_size=10, shuffle=True)\n```\n:::\n\n\nYou can now easily loop through the dataset in batches:\n\n::: {#21510263 .cell execution_count=3}\n``` {.python .cell-code}\nfor batch_idx, (x, y) in enumerate(dataloader):\n    if batch_idx < 2: \n        print(f\"Batch {batch_idx}\")\n        print(\"x:\", x)\n        print(\"y:\", y)\n        print(\"---\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nBatch 0\nx: tensor([39, 59,  1, 31, 25, 13, 98, 43, 38,  8])\ny: tensor([1, 1, 1, 1, 1, 1, 0, 1, 0, 0])\n---\nBatch 1\nx: tensor([89, 88, 40, 93, 60, 68, 67, 24, 42, 15])\ny: tensor([1, 0, 0, 1, 0, 0, 1, 0, 0, 1])\n---\n```\n:::\n:::\n\n\n---\n\n## Step 3: Use a Real-World Dataset (California Housing)\n\nYou can also use built-in datasets from `torchvision`.\n\n::: {#43713452 .cell execution_count=4}\n``` {.python .cell-code}\nfrom sklearn.datasets import fetch_california_housing\nfrom sklearn.preprocessing import StandardScaler\nfrom torch.utils.data import TensorDataset\n\n# Load data\ncalifornia = fetch_california_housing()\nX = california.data[:1000]\ny = california.target[:1000]\n\n# Convert to tensors\nX_tensor = torch.tensor(X, dtype=torch.float32)\ny_tensor = torch.tensor(y, dtype=torch.float32).view(-1, 1)\n\n# Wrap in a dataset\nreal_dataset = TensorDataset(X_tensor, y_tensor)\nreal_loader = DataLoader(real_dataset, batch_size=32, shuffle=True)\n```\n:::\n\n\nPreview the first batch:\n\n::: {#9b0a3299 .cell execution_count=5}\n``` {.python .cell-code}\nfeatures, targets = next(iter(real_loader))\nprint(features.shape)  # [32, 8]\nprint(targets.shape)   # [32, 1]\n```\n\n::: {.cell-output .cell-output-stdout}\n```\ntorch.Size([32, 8])\ntorch.Size([32, 1])\n```\n:::\n:::\n\n\n---\n\n## Step 4: Visualization\n\n::: {#1b2b7278 .cell execution_count=6}\n``` {.python .cell-code}\nimport matplotlib.pyplot as plt\n\nfeature_index = 0  # e.g., Median Income\nplt.hist(features[:, feature_index].numpy(), bins=20, edgecolor='k')\nplt.title(\"Distribution of Feature: Median Income\")\nplt.xlabel(\"Value\")\nplt.ylabel(\"Frequency\")\nplt.grid(True)\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![](Pytorch_dataloader_files/figure-html/cell-7-output-1.png){width=576 height=449}\n:::\n:::\n\n\n---\n\n## Common DataLoader Parameters\n\n| Parameter      | Meaning |\n|----------------|---------|\n| `batch_size`   | Number of samples per batch |\n| `shuffle`      | Whether to shuffle data at every epoch |\n| `num_workers`  | Number of subprocesses to use for data loading |\n| `drop_last`    | Drop last batch if it's smaller than `batch_size` |\n| `pin_memory`   | Speed up data transfer to GPU if using CUDA |\n\n---\n\n## Reproducibility Tip\n\nTo make shuffling deterministic:\n\n```python\ng = torch.Generator()\ng.manual_seed(42)\n\ndataloader = DataLoader(dataset, batch_size=10, shuffle=True, generator=g)\n```\n\n---\n\n## Summary\n\nPyTorch's `DataLoader` is a flexible and powerful abstraction for handling dataset loading. Whether you're using built-in datasets or your own, `DataLoader` gives you:\n\n- Mini-batch loading\n- Shuffling for randomness\n- Parallelism with `num_workers`\n\n",
    "supporting": [
      "Pytorch_dataloader_files"
    ],
    "filters": [],
    "includes": {}
  }
}