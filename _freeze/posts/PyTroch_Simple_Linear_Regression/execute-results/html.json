{
  "hash": "9e202f9e85ee5b1a606932c208337e7a",
  "result": {
    "engine": "jupyter",
    "markdown": "---\ntitle: \"Learning PyTorch: Simple Linear Regression\"\nauthor: \"Kai Tan\"\ndate: \"2024-04-20\"\ncategories: [Python]\nformat:\n  html:\n    code-fold: false\njupyter: python3\n---\n\n\nIn this post, I will demonstrate how to implement a simple linear regression model using PyTorch.\n\n### Data Generation\nGenerate $n=1000$ data points from a linear model:\n$$y_i = b_0 + w_0 * x_i  + \\varepsilon_i$$\n\n- $b_0 = 1$ and $w_0 = 2$\n- $x_i \\sim N(0, 1)$ and $\\varepsilon_i \\sim N(0, 0.01)$\n\n::: {#80aac3e6 .cell execution_count=1}\n``` {.python .cell-code}\nimport numpy as np\nimport matplotlib.pyplot as plt\nrng = np.random.RandomState(42)\nn = 1000\nb0, w0 = 1, 2\nx = rng.standard_normal(size=(n, 1))\ne = 0.1 * rng.standard_normal(size=(n, 1))\ny = b0 + x * w0 + e\n\nprint('true weight:', w0)\nprint('true bias:', b0)\n\nplt.plot(x, y, 'b.')\nplt.xlabel('x')\nplt.ylabel('y')\nplt.title(r'Scatter plot of $y$ vs $x$ with noise')\nplt.show()\n```\n\n::: {.cell-output .cell-output-stdout}\n```\ntrue weight: 2\ntrue bias: 1\n```\n:::\n\n::: {.cell-output .cell-output-display}\n![](PyTroch_Simple_Linear_Regression_files/figure-html/cell-2-output-2.png){width=587 height=450}\n:::\n:::\n\n\n## Solve Linear Regression by Gradient Descent in NumPy\n\n- Step 1: Given weight $w$ and bias $b$, compute prediction (Forward pass): \n$$\\hat{y}_{i} = b + w x_{i}$$\n\n- Step 2: compute the loss:\n$$\nL(w,b)\n= \\frac1n \\sum_{i=1}^n (y_i - \\hat y_i)^2\n= \\frac1n \\sum_{i=1}^n (y_i - b - w * x_i)^2\n$$\n\n- Step 3: compute the gradients (backward pass):\n$$\n\\frac{\\partial L}{\\partial w} \n= -\\frac{2}{n} \\sum_{i=1}^n (y_i - b - w x_i) x_i\n= -\\frac{2}{n} \\sum_{i=1}^n (y_i - \\hat y_i) x_i\n$$\n$$\n\\frac{\\partial L}{\\partial b} \n= -\\frac{2}{n} \\sum_{i=1}^n (y_i - b - w x_i)\n= -\\frac{2}{n} \\sum_{i=1}^n (y_i - \\hat y_i)\n$$\n\n- Step 4: update the parameters:\n$$\nw \\leftarrow w - \\eta \\frac{\\partial L}{\\partial w}$$\n$$\nb \\leftarrow b - \\eta \\frac{\\partial L}{\\partial b}\n$$\n\n::: {#109ee468 .cell execution_count=2}\n``` {.python .cell-code}\n# initialization of weight and bias\nw = rng.standard_normal(1)\nb = rng.standard_normal(1)\nprint('Initial bias:', b)\nprint('Initial weight:', w)\n\nlr = 0.01 # learning rate\nn_epochs = 1000 # number of iterations\nfor t in range(n_epochs):\n    # step 1: prediction\n    y_pred = b + w * x\n    # step 2: compute loss\n    loss = np.mean((y - y_pred) ** 2)\n    # step 3: compute gradients\n    dw = -2 * np.mean(x * (y - y_pred))\n    db = -2 * np.mean(y - y_pred)\n    # step4: update weights and bias\n    w -= lr * dw\n    b -= lr * db\nprint('Final bias:', b)\nprint('Final weight:', w)\n\n# plot the results\nplt.plot(x, y, 'b.')\nplt.plot(x, b + x * w, 'r-')\nplt.xlabel('x')\nplt.ylabel('y')\nplt.title('Linear Regression using Gradient Descent')\nplt.show()\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nInitial bias: [-0.14451867]\nInitial weight: [-0.67517827]\nFinal bias: [1.00716318]\nFinal weight: [1.99588476]\n```\n:::\n\n::: {.cell-output .cell-output-display}\n![](PyTroch_Simple_Linear_Regression_files/figure-html/cell-3-output-2.png){width=587 height=449}\n:::\n:::\n\n\n## Solve Linear Regression with PyTorch (autograd)\n\n::: {#e74c9b66 .cell execution_count=3}\n``` {.python .cell-code}\n# using pytorch\nimport torch\nimport torch.optim as optim\nimport torch.nn as nn\n\ntorch.manual_seed(42)  # for reproducibility\n\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n# convert data to torch tensors\nx_tensor = torch.from_numpy(x).float().to(device)\ny_tensor = torch.from_numpy(y).float().to(device)\n\nlr = 0.1\nn_epochs = 1000\n# initial weights and bias\nb = torch.randn(1, requires_grad=True, device=device)\nw = torch.randn(1, requires_grad=True, device=device)\n\nfor t in range(n_epochs):\n    # step 1: prediction\n    y_hat = b + w * x_tensor\n    # step 2: compute loss\n    error = y_tensor - y_hat\n    loss = torch.mean(error ** 2)\n    # step 3: compute gradients (via autograd)\n    loss.backward()\n    # step 4: update weights and bias\n    with torch.no_grad():\n        b -= lr * b.grad\n        w -= lr * w.grad\n    b.grad.zero_()\n    w.grad.zero_()\nprint('Final bias (PyTorch):', b)\nprint('Final weight (PyTorch):', w)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nFinal bias (PyTorch): tensor([1.0072], requires_grad=True)\nFinal weight (PyTorch): tensor([1.9959], requires_grad=True)\n```\n:::\n:::\n\n\n## Solve Linear Regression with PyTorch (autograd + loss + optimizer)\n\n::: {#fd3b1e86 .cell execution_count=4}\n``` {.python .cell-code}\nb = torch.randn(1, requires_grad=True, dtype=torch.float, device=device)\nw = torch.randn(1, requires_grad=True, dtype=torch.float, device=device)\n\nlr = 0.01\nn_epochs = 1000\nloss_fn = nn.MSELoss(reduction='mean')\noptimizer = optim.SGD([b, w], lr=lr)\nfor t in range(n_epochs):\n    # step 1: prediction\n    y_hat = b + w * x_tensor\n    # step 2: compute loss\n    loss = loss_fn(y_tensor, y_hat)    \n    # step 3: clear old and compute current gradients\n    optimizer.zero_grad() # clear old gradients\n    loss.backward()       # compute current gradients\n    # step 4: update weights and bias using the specified optimizer (e.g. SGD)\n    optimizer.step()\n    \nprint('Final bias (PyTorch with optim):', b)\nprint('Final weight (PyTorch with optim):', w)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nFinal bias (PyTorch with optim): tensor([1.0072], requires_grad=True)\nFinal weight (PyTorch with optim): tensor([1.9959], requires_grad=True)\n```\n:::\n:::\n\n\n## Object-oriented programming\n\n::: {#2339af9c .cell execution_count=5}\n``` {.python .cell-code}\nimport torch\nimport torch.nn as nn\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\nclass LinearRegression(nn.Module):\n    def __init__(self):\n        super().__init__()\n        # Initialize parameters\n        self.b = nn.Parameter(torch.randn(1, device=device)) \n        self.w = nn.Parameter(torch.randn(1, device=device)) \n\n    def forward(self, x):\n        # Forward pass: compute predicted y\n        return self.b + self.w * x \n\n# Create model\nmodel = LinearRegression().to(device)\n# define the loss function and optimizer\nlr = 0.01\nn_epochs = 1000\nloss_fn = nn.MSELoss(reduction='mean')\noptimizer = optim.SGD(model.parameters(), lr=lr)\n\nfor t in range(n_epochs):\n    model.train()\n    # step 1: makd prediction (forward pass)\n    y_hat = model(x_tensor)\n    # step 2: compute loss\n    loss = loss_fn(y_tensor, y_hat)\n    # step 3: clear old and compute current gradients\n    optimizer.zero_grad() # clear old gradients\n    loss.backward()       # compute current gradients\n    # step 4: update parameters via an optimizer (e.g. SGD)\n    optimizer.step() \n\nprint(model.state_dict())\nprint('Final bias (class + PyTorch):', model.b.item())\nprint('Final weight (class + PyTorch):', model.w.item())\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nOrderedDict({'b': tensor([1.0072]), 'w': tensor([1.9959])})\nFinal bias (class + PyTorch): 1.0071604251861572\nFinal weight (class + PyTorch): 1.9958817958831787\n```\n:::\n:::\n\n\n## Create Linear Model via nn.Sequential\n\n::: {#5cd00cfa .cell execution_count=6}\n``` {.python .cell-code}\nlr = 0.01\nn_epochs = 1000\nmodel = nn.Sequential(nn.Linear(1, 1)).to(device)\nloss_fn = nn.MSELoss(reduction='mean')\noptimizer = optim.SGD(model.parameters(), lr=lr)\n\nlosses = []\nfor t in range(n_epochs):\n    model.train()\n    # step 1: make prediction (forward pass)\n    y_hat = model(x_tensor)\n    # step 2: compute loss\n    loss = loss_fn(y_tensor, y_hat)\n    losses.append(loss.item())\n    # step 3: clear old and compute current gradients\n    optimizer.zero_grad() # clear old gradients\n    loss.backward()       # compute current gradients\n    # step 4: update parameters via an optimizer (e.g. SGD)\n    optimizer.step() \n\nprint(model.state_dict())\nplt.plot(losses)\nplt.xlabel('Epochs')\nplt.ylabel('Loss')\nplt.title('Training Loss over Epochs')\nplt.show()\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nOrderedDict({'0.weight': tensor([[1.9959]]), '0.bias': tensor([1.0072])})\n```\n:::\n\n::: {.cell-output .cell-output-display}\n![](PyTroch_Simple_Linear_Regression_files/figure-html/cell-7-output-2.png){width=589 height=449}\n:::\n:::\n\n\n## Dataset and Data Loader\n\n::: {#51768d13 .cell execution_count=7}\n``` {.python .cell-code}\nfrom torch.utils.data import Dataset, TensorDataset\nclass myDataset(Dataset):\n    def __init__(self, x, y):\n        self.x = x_tensor\n        self.y = y_tensor\n    def __getitem__(self, index):\n        return self.x[index], self.y[index]\n    \n    def __len__(self):\n        return len(self.x)\n\ntrain_data = myDataset(x_tensor, y_tensor)\nprint('Length of train_data:', len(train_data))\nprint('First item in train_data:', train_data[0])\n\ntrain_data1 = TensorDataset(x_tensor, y_tensor)\nprint('Length of train_data1:', len(train_data1))\nprint('First item in train_data1:', train_data1[0])\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nLength of train_data: 1000\nFirst item in train_data: (tensor([0.4967]), tensor([2.1334]))\nLength of train_data1: 1000\nFirst item in train_data1: (tensor([0.4967]), tensor([2.1334]))\n```\n:::\n:::\n\n\n## Solve Linear Regression with Dataset and Data Loader\n\n::: {#98b12f19 .cell execution_count=8}\n``` {.python .cell-code}\nfrom torch.utils.data import DataLoader\ntrain_loader = DataLoader(train_data, batch_size=50, shuffle=True)\nlr = 0.01\nn_epochs = 1000\nmodel = nn.Sequential(nn.Linear(1, 1)).to(device)\nloss_fn = nn.MSELoss(reduction='mean')\noptimizer = optim.SGD(model.parameters(), lr=lr)\nfor t in range(n_epochs):\n    for x_batch, y_batch in train_loader:\n        x_batch = x_batch.to(device)\n        y_batch = y_batch.to(device)\n        model.train()\n        # step 1: make prediction (forward pass)\n        y_hat = model(x_batch)\n        # step 2: compute loss\n        loss = loss_fn(y_batch, y_hat)\n        # step 3: clear old and compute current gradients\n        optimizer.zero_grad() # clear old gradients\n        loss.backward()       # compute current gradients\n        # step 4: update parameters via an optimizer (e.g. SGD)\n        optimizer.step()       \nprint(model.state_dict())\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nOrderedDict({'0.weight': tensor([[1.9958]]), '0.bias': tensor([1.0073])})\n```\n:::\n:::\n\n\n## Evaluation in validation set\n\n::: {#9e9a813c .cell execution_count=9}\n``` {.python .cell-code}\nfrom torch.utils.data import random_split\ndataset = TensorDataset(x_tensor, y_tensor)\ntrain_dataset, val_dataset = random_split(dataset, [800, 200])\ntrain_loader = DataLoader(train_dataset, batch_size=40)\nval_loader = DataLoader(val_dataset, batch_size=20)\nlosses = []\nval_losses = []\n\nfor t in range(n_epochs):\n    for x_batch, y_batch in train_loader:\n        x_batch = x_batch.to(device)\n        y_batch = y_batch.to(device)\n\n        model.train()\n        # step 1: make prediction (forward pass)\n        y_hat = model(x_batch)\n        # step 2: compute loss\n        loss = loss_fn(y_batch, y_hat)\n        # step 3: clear old and compute current gradients\n        optimizer.zero_grad() # clear old gradients\n        loss.backward()       # compute current gradients\n        # step 4: update parameters via an optimizer (e.g. SGD)\n        optimizer.step() \n        \n        # store the loss for plotting later\n        losses.append(loss)\n    \n    # validation step (no need to track gradients)\n    with torch.no_grad():\n        for x_val, y_val in val_loader:\n            model.eval()\n            # step 1: prediction\n            y_hat = model(x_val.to(device))\n            # step 2: compute loss\n            val_loss = loss_fn(y_val.to(device), y_hat)\n            val_losses.append(val_loss.item())\nprint(model.state_dict())\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nOrderedDict({'0.weight': tensor([[1.9960]]), '0.bias': tensor([1.0072])})\n```\n:::\n:::\n\n\n## Summary\n- No need to manually compute gradients; PyTorch's autograd does it for you.\n- Use `torch.nn` for loss functions and optimizers.\n- Use `nn.Sequential` for building models in a modular way.\n- Use `torch.utils.data.Dataset` and `DataLoader` for data handling.\n- Use validation sets to evaluate model performance.\n- [Reference](https://huggingface.co/blog/dvgodoy/beginner-pytorch-tutorial#a-beginner-friendly-pytorch-tutorial-build-and-train-your-first-model)\n\n",
    "supporting": [
      "PyTroch_Simple_Linear_Regression_files"
    ],
    "filters": [],
    "includes": {}
  }
}